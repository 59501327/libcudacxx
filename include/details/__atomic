/*

Copyright (c) 2018, NVIDIA Corporation

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

*/

#include <assert.h>

#ifndef __GCC_ATOMIC_BOOL_LOCK_FREE
    #define __GCC_ATOMIC_BOOL_LOCK_FREE 2
    #define __GCC_ATOMIC_CHAR_LOCK_FREE 2
    #define __GCC_ATOMIC_CHAR16_T_LOCK_FREE 2
    #define __GCC_ATOMIC_CHAR32_T_LOCK_FREE 2
    #define __GCC_ATOMIC_WCHAR_T_LOCK_FREE 2
    #define __GCC_ATOMIC_SHORT_LOCK_FREE 2
    #define __GCC_ATOMIC_INT_LOCK_FREE 2
    #define __GCC_ATOMIC_LONG_LOCK_FREE 2
    #define __GCC_ATOMIC_LLONG_LOCK_FREE 2
    #define __GCC_ATOMIC_POINTER_LOCK_FREE 2
#endif

#ifndef __ATOMIC_RELAXED
    #define __ATOMIC_RELAXED 0
    #define __ATOMIC_CONSUME 1
    #define __ATOMIC_ACQUIRE 2
    #define __ATOMIC_RELEASE 3
    #define __ATOMIC_ACQ_REL 4
    #define __ATOMIC_SEQ_CST 5
#endif

namespace simt { namespace details { inline namespace v1 {

typedef ::std::memory_order memory_order;

static constexpr memory_order memory_order_relaxed = ::std::memory_order_relaxed;
static constexpr memory_order memory_order_release = ::std::memory_order_release;
static constexpr memory_order memory_order_acq_rel = ::std::memory_order_acq_rel;
static constexpr memory_order memory_order_acquire = ::std::memory_order_acquire;
static constexpr memory_order memory_order_consume = ::std::memory_order_consume;
static constexpr memory_order memory_order_seq_cst = ::std::memory_order_seq_cst;

inline __device__ int __stronger_order_simt(int a, int b) {
    int const max = a > b ? a : b;
    if(max != __ATOMIC_RELEASE)
        return max;
    static int const xform[] = { 
        __ATOMIC_RELEASE, 
        __ATOMIC_ACQ_REL, 
        __ATOMIC_ACQ_REL, 
        __ATOMIC_RELEASE };
    return xform[a < b ? a : b];
}

struct __memory_scope_block;
struct __memory_scope_device;
struct __memory_scope_system;

#define __memory_scope_default ::simt::details::__memory_scope_system

} } }

#if defined(__CUDA_ARCH__)

#include "__atomic_generated"
#include "__atomic_derived"

namespace simt { namespace details { inline namespace v1 {

template <typename _Tp, typename _Sco>
struct __c11_atomic_wrapper {

  __host__ __device__ __c11_atomic_wrapper() : __a_value() {}
  __host__ __device__ constexpr explicit __c11_atomic_wrapper(_Tp value) : __a_value(value) {}
  _Tp __a_value;
};

template <typename _Tp, typename _Sco> static __host__ __device__ inline constexpr 
_Sco __get_scope(__c11_atomic_wrapper<_Tp, _Sco> const*) { return _Sco(); }
template <typename _Tp, typename _Sco> static __host__ __device__ inline constexpr 
_Sco __get_scope(__c11_atomic_wrapper<_Tp, _Sco> *) { return _Sco(); }
template <typename _Tp, typename _Sco> static __host__ __device__ inline constexpr 
_Sco __get_scope(__c11_atomic_wrapper<_Tp, _Sco> volatile const*) { return _Sco(); }
template <typename _Tp, typename _Sco> static __host__ __device__ inline constexpr 
_Sco __get_scope(__c11_atomic_wrapper<_Tp, _Sco> volatile *) { return _Sco(); }

template <typename _Tp, typename _Sco> static __host__ __device__ inline constexpr 
_Tp const* __get_value(__c11_atomic_wrapper<_Tp, _Sco> const* ptr) { return &ptr->__a_value; }
template <typename _Tp, typename _Sco> static __host__ __device__ inline constexpr 
_Tp* __get_value(__c11_atomic_wrapper<_Tp, _Sco> * ptr) { return &ptr->__a_value; }
template <typename _Tp, typename _Sco> static __host__ __device__ inline constexpr 
/*volatile*/ _Tp const* __get_value(__c11_atomic_wrapper<_Tp, _Sco> volatile const* ptr) { 
    return const_cast<_Tp const*>(&ptr->__a_value); }
template <typename _Tp, typename _Sco> static __host__ __device__ inline constexpr 
/*volatile*/ _Tp * __get_value(__c11_atomic_wrapper<_Tp, _Sco> volatile * ptr) { 
    return const_cast<_Tp*>(&ptr->__a_value); }

} } }

#define _Atomic_s(_Tp, _Sco) ::simt::details::__c11_atomic_wrapper<_Tp, _Sco>
#define _Atomic(_Tp) _Atomic_s(_Tp, __memory_scope_default)

#define __c11_atomic_is_lock_free(x) \
    (x <= 8)
#define __c11_atomic_init(__a, __val)
#define __c11_atomic_thread_fence(__order) \
    ::simt::details::__atomic_thread_fence_simt(__order, __memory_scope_default())
#define __c11_atomic_signal_fence(__order) \
    ::simt::details::__atomic_signal_fence_simt(__order)
#define __c11_atomic_store(__a, __val, __order) \
    ::simt::details::__atomic_store_n_simt(::simt::details::__get_value(__a), __val, __order, ::simt::details::__get_scope(__a))
#define __c11_atomic_load(__a, __order) \
    ::simt::details::__atomic_load_n_simt(::simt::details::__get_value(__a), __order, ::simt::details::__get_scope(__a))
#define __c11_atomic_exchange(__a, __value, __order) \
    ::simt::details::__atomic_exchange_n_simt(::simt::details::__get_value(__a), __value, __order, ::simt::details::__get_scope(__a))
#define __c11_atomic_compare_exchange_strong(__a, __expected, __value, __success, __failure) \
    ::simt::details::__atomic_compare_exchange_n_simt(::simt::details::__get_value(__a), __expected, __value, false, __success, __failure, ::simt::details::__get_scope(__a))
#define __c11_atomic_compare_exchange_weak(__a, __expected, __value, __success, __failure) \
    ::simt::details::__atomic_compare_exchange_n_simt(::simt::details::__get_value(__a), __expected, __value, true, __success, __failure, ::simt::details::__get_scope(__a))
#define __c11_atomic_fetch_add(__a, __delta, __order) \
    ::simt::details::__atomic_fetch_add_simt(::simt::details::__get_value(__a), __delta, __order, ::simt::details::__get_scope(__a))
#define __c11_atomic_fetch_sub(__a, __delta, __order) \
    ::simt::details::__atomic_fetch_sub_simt(::simt::details::__get_value(__a), __delta, __order, ::simt::details::__get_scope(__a))
#define __c11_atomic_fetch_and(__a, __pattern, __order) \
    ::simt::details::__atomic_fetch_and_simt(::simt::details::__get_value(__a), __pattern, __order, ::simt::details::__get_scope(__a))
#define __c11_atomic_fetch_or(__a, __pattern, __order) \
    ::simt::details::__atomic_fetch_or_simt(::simt::details::__get_value(__a), __pattern, __order, ::simt::details::__get_scope(__a))
#define __c11_atomic_fetch_xor(__a, __pattern, __order) \
    ::simt::details::__atomic_fetch_xor_simt(::simt::details::__get_value(__a), __pattern, __order, ::simt::details::__get_scope(__a))

#elif defined(_MSC_VER)

#define _Atomic_s(_Tp, _Sco) ::std::atomic<_Tp>
#define _Atomic(_Tp) _Atomic_s(_Tp, __memory_scope_default)

static inline void __std_atomic_thread_fence(std::memory_order __order) {
    std::atomic_thread_fence(__order);
}
static inline void __std_atomic_signal_fence(std::memory_order __order) {
    std::atomic_signal_fence(__order);
}

#define __c11_atomic_is_lock_free(x) \
    (x <= 8)
#define __c11_atomic_init(__a, __val)
#define __c11_atomic_thread_fence(__order) \
    __std_atomic_thread_fence((::std::memory_order)__order)
#define __c11_atomic_signal_fence(__order) \
    __std_atomic_signal_fence((::std::memory_order)__order)
#define __c11_atomic_store(__a, __val, __order) \
    ::std::atomic_store_explicit(__a, __val, (::std::memory_order)__order)
#define __c11_atomic_load(__a, __order) \
    ::std::atomic_load_explicit(__a, (::std::memory_order)__order)
#define __c11_atomic_exchange(__a, __value, __order) \
    ::std::atomic_exchange_explicit(__a, __value, (::std::memory_order)__order)
#define __c11_atomic_compare_exchange_strong(__a, __expected, __value, __success, __failure) \
    ::std::atomic_compare_exchange_strong_explicit(__a, __expected, __value, (::std::memory_order)__success, (::std::memory_order)__failure)
#define __c11_atomic_compare_exchange_weak(__a, __expected, __value, __success, __failure) \
    ::std::atomic_compare_exchange_weak_explicit(__a, __expected, __value, (::std::memory_order)__success, (::std::memory_order)__failure)
#define __c11_atomic_fetch_add(__a, __delta, __order) \
    ::std::atomic_fetch_add_explicit(__a, __delta, (::std::memory_order)__order)
#define __c11_atomic_fetch_sub(__a, __delta, __order) \
    ::std::atomic_fetch_sub_explicit(__a, __delta, (::std::memory_order)__order)
#define __c11_atomic_fetch_and(__a, __pattern, __order) \
    ::std::atomic_fetch_and_explicit(__a, __pattern, (::std::memory_order)__order)
#define __c11_atomic_fetch_or(__a, __pattern, __order) \
    ::std::atomic_fetch_or_explicit(__a, __pattern, (::std::memory_order)__order)
#define __c11_atomic_fetch_xor(__a, __pattern, __order) \
    ::std::atomic_fetch_xor_explicit(__a, __pattern, (::std::memory_order)__order)

#endif
