/*

Copyright (c) 2018, NVIDIA Corporation

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

*/

#include <assert.h>

#ifndef __GCC_ATOMIC_BOOL_LOCK_FREE
    #define __GCC_ATOMIC_BOOL_LOCK_FREE 2
    #define __GCC_ATOMIC_CHAR_LOCK_FREE 2
    #define __GCC_ATOMIC_CHAR16_T_LOCK_FREE 2
    #define __GCC_ATOMIC_CHAR32_T_LOCK_FREE 2
    #define __GCC_ATOMIC_WCHAR_T_LOCK_FREE 2
    #define __GCC_ATOMIC_SHORT_LOCK_FREE 2
    #define __GCC_ATOMIC_INT_LOCK_FREE 2
    #define __GCC_ATOMIC_LONG_LOCK_FREE 2
    #define __GCC_ATOMIC_LLONG_LOCK_FREE 2
    #define __GCC_ATOMIC_POINTER_LOCK_FREE 2
#endif

#ifndef __ATOMIC_RELAXED
    #define __ATOMIC_RELAXED 0
    #define __ATOMIC_CONSUME 1
    #define __ATOMIC_ACQUIRE 2
    #define __ATOMIC_RELEASE 3
    #define __ATOMIC_ACQ_REL 4
    #define __ATOMIC_SEQ_CST 5
#endif

namespace simt { namespace details { inline namespace v1 {

enum memory_order {
    memory_order_relaxed = __ATOMIC_RELAXED,
    memory_order_release = __ATOMIC_CONSUME,
    memory_order_acq_rel = __ATOMIC_ACQUIRE,
    memory_order_acquire = __ATOMIC_RELEASE,
    memory_order_consume = __ATOMIC_ACQ_REL,
    memory_order_seq_cst = __ATOMIC_SEQ_CST
};

inline __device__ int __stronger_order_simt(int a, int b) {
    int const max = a > b ? a : b;
    if(max != __ATOMIC_RELEASE)
        return max;
    static int const xform[] = { 
        __ATOMIC_RELEASE, 
        __ATOMIC_ACQ_REL, 
        __ATOMIC_ACQ_REL, 
        __ATOMIC_RELEASE };
    return xform[a < b ? a : b];
}

struct __memory_scope_block;
struct __memory_scope_device;
struct __memory_scope_system;

#define _LIBCPP_ATOMIC_SCOPE_DEFAULT ::simt::details::__memory_scope_system

} } }

#if defined(__CUDA_ARCH__)

#if defined(__clang__)
    #define __c11_atomic_is_lock_free __cxx_atomic_is_lock_free
    #define __c11_atomic_init __cxx_atomic_init
    #define __c11_atomic_thread_fence __cxx_atomic_thread_fence
    #define __c11_atomic_signal_fence __cxx_atomic_signal_fence
    #define __c11_atomic_store __cxx_atomic_store
    #define __c11_atomic_load __cxx_atomic_load
    #define __c11_atomic_exchange __cxx_atomic_exchange
    #define __c11_atomic_compare_exchange_strong __cxx_atomic_compare_exchange_strong
    #define __c11_atomic_compare_exchange_weak __cxx_atomic_compare_exchange_weak
    #define __c11_atomic_fetch_add __cxx_atomic_fetch_add
    #define __c11_atomic_fetch_sub __cxx_atomic_fetch_sub
    #define __c11_atomic_fetch_and __cxx_atomic_fetch_and
    #define __c11_atomic_fetch_or __cxx_atomic_fetch_or
    #define __c11_atomic_fetch_xor __cxx_atomic_fetch_xor
#endif

#include "__atomic_generated"
#include "__atomic_derived"

namespace simt { namespace details { inline namespace v1 {

template <typename _Tp, typename _Sco>
struct __c11_atomic_wrapper {

  __host__ __device__ __c11_atomic_wrapper() : __a_value() {}
  __host__ __device__ constexpr explicit __c11_atomic_wrapper(_Tp value) : __a_value(value) {}
  _Tp __a_value;
};

#define _Atomic_s(_Tp, _Sco) ::simt::details::__c11_atomic_wrapper<_Tp, _Sco>
#define _Atomic(_Tp) _Atomic_s(_Tp, ::simt::details::__memory_scope_system)

template<class _Tp, class _Sco> 
__device__ inline void __c11_atomic_init(__c11_atomic_wrapper<_Tp, _Sco> volatile*, _Tp) { }
template<class _Tp, class _Sco> 
__device__ inline void __c11_atomic_store(__c11_atomic_wrapper<_Tp, _Sco> volatile* __a, _Tp __val, int __order) {
    __atomic_store_n_simt(&__a->__a_value, __val, __order, _Sco()); }
template<class _Tp, class _Sco> 
__device__ inline _Tp __c11_atomic_load(__c11_atomic_wrapper<_Tp, _Sco> volatile* __a, int __order) {
    return __atomic_load_n_simt(&__a->__a_value, __order, _Sco()); }
template<class _Tp, class _Sco> 
__device__ inline _Tp __c11_atomic_exchange(__c11_atomic_wrapper<_Tp, _Sco> volatile* __a, _Tp __value, int __order) {
    return __atomic_exchange_n_simt(&__a->__a_value, __value, __order, _Sco()); }
template<class _Tp, class _Sco> 
__device__ inline bool __c11_atomic_compare_exchange_strong(__c11_atomic_wrapper<_Tp, _Sco> volatile* __a, _Tp __expected, _Tp __value, int __success, int __failure) {
    return __atomic_compare_exchange_n_simt(&__a->__a_value, __expected, __value, false, (::simt::details::memory_order)__success, (::simt::details::memory_order)__failure, _Sco()); }
template<class _Tp, class _Sco> 
__device__ inline bool __c11_atomic_compare_exchange_weak(__c11_atomic_wrapper<_Tp, _Sco> volatile* __a, _Tp __expected, _Tp __value, int __success, int __failure) {
    return __atomic_compare_exchange_n_simt(&__a->__a_value, __expected, __value, true, (::simt::details::memory_order)__success, (::simt::details::memory_order)__failure, _Sco()); }
template<class _Tp, class _Sco> 
__device__ inline _Tp __c11_atomic_fetch_add(__c11_atomic_wrapper<_Tp, _Sco> volatile* __a, _Tp __delta, int __order) {
    return __atomic_fetch_add_simt(&__a->__a_value, __delta, __order, _Sco()); }
template<class _Tp, class _Sco> 
__device__ inline _Tp __c11_atomic_fetch_sub(__c11_atomic_wrapper<_Tp, _Sco> volatile* __a, _Tp __delta, int __order) {
    return __atomic_fetch_sub_simt(&__a->__a_value, __delta, __order, _Sco()); }
template<class _Tp, class _Sco> 
__device__ inline _Tp __c11_atomic_fetch_and(__c11_atomic_wrapper<_Tp, _Sco> volatile* __a, _Tp __pattern, int __order) {
    return __atomic_fetch_and_simt(&__a->__a_value, __pattern, __order, _Sco()); }
template<class _Tp, class _Sco> 
__device__ inline _Tp __c11_atomic_fetch_or(__c11_atomic_wrapper<_Tp, _Sco> volatile* __a, _Tp __pattern, int __order) {
    return __atomic_fetch_or_simt(&__a->__a_value, __pattern, __order, _Sco()); }
template<class _Tp, class _Sco> 
__device__ inline _Tp __c11_atomic_fetch_xor(__c11_atomic_wrapper<_Tp, _Sco> volatile* __a, _Tp __pattern, int __order) {
    return __atomic_fetch_xor_simt(&__a->__a_value, __pattern, __order, _Sco()); }
template<class _Tp, class _Sco> 
__device__ inline _Tp* __c11_atomic_fetch_add(__c11_atomic_wrapper<_Tp*, _Sco> volatile* __a, ptrdiff_t __delta, int __order) {
    return __atomic_fetch_add_simt(&__a->__a_value, __delta, __order, _Sco()); }

} } }

__device__ inline bool __c11_atomic_is_lock_free(size_t x) { return x <= 8; }
__device__ inline void __c11_atomic_thread_fence(int __order) {
    ::simt::details::__atomic_thread_fence_simt(__order, ::simt::details::__memory_scope_system()); }
__device__ inline void __c11_atomic_signal_fence(int __order) {
    ::simt::details::__atomic_signal_fence_simt(__order); }

#elif defined(_MSC_VER)

#define _Atomic_s(_Tp, _Sco) ::std::atomic<_Tp>
#define _Atomic(_Tp) _Atomic_s(_Tp, __memory_scope_default)

static inline void __std_atomic_thread_fence(std::memory_order __order) {
    std::atomic_thread_fence(__order);
}
static inline void __std_atomic_signal_fence(std::memory_order __order) {
    std::atomic_signal_fence(__order);
}

#define __c11_atomic_is_lock_free(x) \
    (x <= 8)
#define __c11_atomic_init(__a, __val)
#define __c11_atomic_thread_fence(__order) \
    __std_atomic_thread_fence((::std::memory_order)__order)
#define __c11_atomic_signal_fence(__order) \
    __std_atomic_signal_fence((::std::memory_order)__order)
#define __c11_atomic_store(__a, __val, __order) \
    ::std::atomic_store_explicit(__a, __val, (::std::memory_order)__order)
#define __c11_atomic_load(__a, __order) \
    ::std::atomic_load_explicit(__a, (::std::memory_order)__order)
#define __c11_atomic_exchange(__a, __value, __order) \
    ::std::atomic_exchange_explicit(__a, __value, (::std::memory_order)__order)
#define __c11_atomic_compare_exchange_strong(__a, __expected, __value, __success, __failure) \
    ::std::atomic_compare_exchange_strong_explicit(__a, __expected, __value, (::std::memory_order)__success, (::std::memory_order)__failure)
#define __c11_atomic_compare_exchange_weak(__a, __expected, __value, __success, __failure) \
    ::std::atomic_compare_exchange_weak_explicit(__a, __expected, __value, (::std::memory_order)__success, (::std::memory_order)__failure)
#define __c11_atomic_fetch_add(__a, __delta, __order) \
    ::std::atomic_fetch_add_explicit(__a, __delta, (::std::memory_order)__order)
#define __c11_atomic_fetch_sub(__a, __delta, __order) \
    ::std::atomic_fetch_sub_explicit(__a, __delta, (::std::memory_order)__order)
#define __c11_atomic_fetch_and(__a, __pattern, __order) \
    ::std::atomic_fetch_and_explicit(__a, __pattern, (::std::memory_order)__order)
#define __c11_atomic_fetch_or(__a, __pattern, __order) \
    ::std::atomic_fetch_or_explicit(__a, __pattern, (::std::memory_order)__order)
#define __c11_atomic_fetch_xor(__a, __pattern, __order) \
    ::std::atomic_fetch_xor_explicit(__a, __pattern, (::std::memory_order)__order)

#endif
