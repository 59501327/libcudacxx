/*

Copyright (c) 2018, NVIDIA Corporation

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

*/

#ifndef _SIMT_DETAILS_CONFIG
#define _SIMT_DETAILS_CONFIG

struct __atomic_scope_system { };
struct __atomic_scope_device { };
struct __atomic_scope_block { };

using __atomic_scope_default = __atomic_scope_system;

#ifdef __CUDA_ARCH__

#define __simt_fence_signal_() asm volatile("":::"memory")

#define __simt_fence_sc_system_() asm volatile("fence.sc.sys;":::"memory")
#define __simt_fence_system_() asm volatile("fence.sys;":::"memory")
#define __simt_fence_sc_device_() asm volatile("fence.sc.gpu;":::"memory")

#define __simt_fence_device_() asm volatile("fence.gpu;":::"memory")
#define __simt_fence_sc_block_() asm volatile("fence.sc.cta;":::"memory")
#define __simt_fence_block_() asm volatile("fence.cta;":::"memory")


#define __simt_load_acquire_8_as_32_system(ptr,ret) asm volatile("ld.acquire.sys.b8 %0, [%1];" : "=r"(ret) : "l"(ptr) : "memory")
#define __simt_load_relaxed_8_as_32_system(ptr,ret) asm volatile("ld.relaxed.sys.b8 %0, [%1];" : "=r"(ret) : "l"(ptr) : "memory")
#define __simt_load_acquire_8_as_32_device(ptr,ret) asm volatile("ld.acquire.gpu.b8 %0, [%1];" : "=r"(ret) : "l"(ptr) : "memory")
#define __simt_load_relaxed_8_as_32_device(ptr,ret) asm volatile("ld.relaxed.gpu.b8 %0, [%1];" : "=r"(ret) : "l"(ptr) : "memory")
#define __simt_load_acquire_8_as_32_block(ptr,ret) asm volatile("ld.acquire.cta.b8 %0, [%1];" : "=r"(ret) : "l"(ptr) : "memory")
#define __simt_load_relaxed_8_as_32_block(ptr,ret) asm volatile("ld.relaxed.cta.b8 %0, [%1];" : "=r"(ret) : "l"(ptr) : "memory")

#define __simt_store_release_8_as_32_system(ptr,desired) asm volatile("st.release.sys.b8 [%0], %1;" :: "l"(ptr), "r"(desired) : "memory")
#define __simt_store_relaxed_8_as_32_system(ptr,desired) asm volatile("st.relaxed.sys.b8 [%0], %1;" :: "l"(ptr), "r"(desired) : "memory")
#define __simt_store_release_8_as_32_device(ptr,desired) asm volatile("st.release.gpu.b8 [%0], %1;" :: "l"(ptr), "r"(desired) : "memory")
#define __simt_store_relaxed_8_as_32_device(ptr,desired) asm volatile("st.relaxed.gpu.b8 [%0], %1;" :: "l"(ptr), "r"(desired) : "memory")
#define __simt_store_release_8_as_32_block(ptr,desired) asm volatile("st.release.cta.b8 [%0], %1;" :: "l"(ptr), "r"(desired) : "memory")
#define __simt_store_relaxed_8_as_32_block(ptr,desired) asm volatile("st.relaxed.cta.b8 [%0], %1;" :: "l"(ptr), "r"(desired) : "memory")


#define __simt_load_acquire_16_system(ptr,ret) asm volatile("ld.acquire.sys.b16 %0, [%1];" : "=h"(ret) : "l"(ptr) : "memory")
#define __simt_load_relaxed_16_system(ptr,ret) asm volatile("ld.relaxed.sys.b16 %0, [%1];" : "=h"(ret) : "l"(ptr) : "memory")
#define __simt_load_acquire_16_device(ptr,ret) asm volatile("ld.acquire.gpu.b16 %0, [%1];" : "=h"(ret) : "l"(ptr) : "memory")
#define __simt_load_relaxed_16_device(ptr,ret) asm volatile("ld.relaxed.gpu.b16 %0, [%1];" : "=h"(ret) : "l"(ptr) : "memory")
#define __simt_load_acquire_16_block(ptr,ret) asm volatile("ld.acquire.cta.b16 %0, [%1];" : "=h"(ret) : "l"(ptr) : "memory")
#define __simt_load_relaxed_16_block(ptr,ret) asm volatile("ld.relaxed.cta.b16 %0, [%1];" : "=h"(ret) : "l"(ptr) : "memory")

#define __simt_store_release_16_system(ptr,desired) asm volatile("st.release.sys.b16 [%0], %1;" :: "l"(ptr), "h"(desired) : "memory")
#define __simt_store_relaxed_16_system(ptr,desired) asm volatile("st.relaxed.sys.b16 [%0], %1;" :: "l"(ptr), "h"(desired) : "memory")
#define __simt_store_release_16_device(ptr,desired) asm volatile("st.release.gpu.b16 [%0], %1;" :: "l"(ptr), "h"(desired) : "memory")
#define __simt_store_relaxed_16_device(ptr,desired) asm volatile("st.relaxed.gpu.b16 [%0], %1;" :: "l"(ptr), "h"(desired) : "memory")
#define __simt_store_release_16_block(ptr,desired) asm volatile("st.release.cta.b16 [%0], %1;" :: "l"(ptr), "h"(desired) : "memory")
#define __simt_store_relaxed_16_block(ptr,desired) asm volatile("st.relaxed.cta.b16 [%0], %1;" :: "l"(ptr), "h"(desired) : "memory")


#define __simt_load_acquire_32_system(ptr,ret) asm volatile("ld.acquire.sys.b32 %0, [%1];" : "=r"(ret) : "l"(ptr) : "memory")
#define __simt_load_relaxed_32_system(ptr,ret) asm volatile("ld.relaxed.sys.b32 %0, [%1];" : "=r"(ret) : "l"(ptr) : "memory")
#define __simt_load_acquire_32_device(ptr,ret) asm volatile("ld.acquire.gpu.b32 %0, [%1];" : "=r"(ret) : "l"(ptr) : "memory")
#define __simt_load_relaxed_32_device(ptr,ret) asm volatile("ld.relaxed.gpu.b32 %0, [%1];" : "=r"(ret) : "l"(ptr) : "memory")
#define __simt_load_acquire_32_block(ptr,ret) asm volatile("ld.acquire.block.b32 %0, [%1];" : "=r"(ret) : "l"(ptr) : "memory")
#define __simt_load_relaxed_32_block(ptr,ret) asm volatile("ld.relaxed.block.b32 %0, [%1];" : "=r"(ret) : "l"(ptr) : "memory")

#define __simt_store_release_32_system(ptr,desired) asm volatile("st.release.sys.b32 [%0], %1;" :: "l"(ptr), "r"(desired) : "memory")
#define __simt_store_relaxed_32_system(ptr,desired) asm volatile("st.relaxed.sys.b32 [%0], %1;" :: "l"(ptr), "r"(desired) : "memory")
#define __simt_store_release_32_device(ptr,desired) asm volatile("st.release.gpu.b32 [%0], %1;" :: "l"(ptr), "r"(desired) : "memory")
#define __simt_store_relaxed_32_device(ptr,desired) asm volatile("st.relaxed.gpu.b32 [%0], %1;" :: "l"(ptr), "r"(desired) : "memory")
#define __simt_store_release_32_block(ptr,desired) asm volatile("st.release.cta.b32 [%0], %1;" :: "l"(ptr), "r"(desired) : "memory")
#define __simt_store_relaxed_32_block(ptr,desired) asm volatile("st.relaxed.cta.b32 [%0], %1;" :: "l"(ptr), "r"(desired) : "memory")

#define __simt_exch_release_32_system(ptr,old,desired) asm volatile("atom.exch.release.sys.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(desired) : "memory")
#define __simt_exch_acquire_32_system(ptr,old,desired) asm volatile("atom.exch.acquire.sys.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(desired) : "memory")
#define __simt_exch_acq_rel_32_system(ptr,old,desired) asm volatile("atom.exch.acq_rel.sys.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(desired) : "memory")
#define __simt_exch_relaxed_32_system(ptr,old,desired) asm volatile("atom.exch.relaxed.sys.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(desired) : "memory")
#define __simt_exch_release_32_device(ptr,old,desired) asm volatile("atom.exch.release.gpu.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(desired) : "memory")
#define __simt_exch_acquire_32_device(ptr,old,desired) asm volatile("atom.exch.acquire.gpu.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(desired) : "memory")
#define __simt_exch_acq_rel_32_device(ptr,old,desired) asm volatile("atom.exch.acq_rel.gpu.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(desired) : "memory")
#define __simt_exch_relaxed_32_device(ptr,old,desired) asm volatile("atom.exch.relaxed.gpu.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(desired) : "memory")
#define __simt_exch_release_32_block(ptr,old,desired) asm volatile("atom.exch.release.cta.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(desired) : "memory")
#define __simt_exch_acquire_32_block(ptr,old,desired) asm volatile("atom.exch.acquire.cta.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(desired) : "memory")
#define __simt_exch_acq_rel_32_block(ptr,old,desired) asm volatile("atom.exch.acq_rel.cta.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(desired) : "memory")
#define __simt_exch_relaxed_32_block(ptr,old,desired) asm volatile("atom.exch.relaxed.cta.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(desired) : "memory")

#define __simt_cas_release_32_system(ptr,old,expected,desired) asm volatile("atom.cas.release.sys.b32 %0, [%1], %2, %3;" : "=r"(old) : "l"(ptr), "r"(expected), "r"(desired) : "memory")
#define __simt_cas_acquire_32_system(ptr,old,expected,desired) asm volatile("atom.cas.acquire.sys.b32 %0, [%1], %2, %3;" : "=r"(old) : "l"(ptr), "r"(expected), "r"(desired) : "memory")
#define __simt_cas_acq_rel_32_system(ptr,old,expected,desired) asm volatile("atom.cas.acq_rel.sys.b32 %0, [%1], %2, %3;" : "=r"(old) : "l"(ptr), "r"(expected), "r"(desired) : "memory")
#define __simt_cas_relaxed_32_system(ptr,old,expected,desired) asm volatile("atom.cas.relaxed.sys.b32 %0, [%1], %2, %3;" : "=r"(old) : "l"(ptr), "r"(expected), "r"(desired) : "memory")
#define __simt_cas_release_32_device(ptr,old,expected,desired) asm volatile("atom.cas.release.gpu.b32 %0, [%1], %2, %3;" : "=r"(old) : "l"(ptr), "r"(expected), "r"(desired) : "memory")
#define __simt_cas_acquire_32_device(ptr,old,expected,desired) asm volatile("atom.cas.acquire.gpu.b32 %0, [%1], %2, %3;" : "=r"(old) : "l"(ptr), "r"(expected), "r"(desired) : "memory")
#define __simt_cas_acq_rel_32_device(ptr,old,expected,desired) asm volatile("atom.cas.acq_rel.gpu.b32 %0, [%1], %2, %3;" : "=r"(old) : "l"(ptr), "r"(expected), "r"(desired) : "memory")
#define __simt_cas_relaxed_32_device(ptr,old,expected,desired) asm volatile("atom.cas.relaxed.gpu.b32 %0, [%1], %2, %3;" : "=r"(old) : "l"(ptr), "r"(expected), "r"(desired) : "memory")
#define __simt_cas_release_32_block(ptr,old,expected,desired) asm volatile("atom.cas.release.cta.b32 %0, [%1], %2, %3;" : "=r"(old) : "l"(ptr), "r"(expected), "r"(desired) : "memory")
#define __simt_cas_acquire_32_block(ptr,old,expected,desired) asm volatile("atom.cas.acquire.cta.b32 %0, [%1], %2, %3;" : "=r"(old) : "l"(ptr), "r"(expected), "r"(desired) : "memory")
#define __simt_cas_acq_rel_32_block(ptr,old,expected,desired) asm volatile("atom.cas.acq_rel.cta.b32 %0, [%1], %2, %3;" : "=r"(old) : "l"(ptr), "r"(expected), "r"(desired) : "memory")
#define __simt_cas_relaxed_32_block(ptr,old,expected,desired) asm volatile("atom.cas.relaxed.cta.b32 %0, [%1], %2, %3;" : "=r"(old) : "l"(ptr), "r"(expected), "r"(desired) : "memory")

#define __simt_add_release_32_system(ptr,old,addend) asm volatile("atom.add.release.sys.u32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(addend) : "memory")
#define __simt_add_acquire_32_system(ptr,old,addend) asm volatile("atom.add.acquire.sys.u32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(addend) : "memory")
#define __simt_add_acq_rel_32_system(ptr,old,addend) asm volatile("atom.add.acq_rel.sys.u32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(addend) : "memory")
#define __simt_add_relaxed_32_system(ptr,old,addend) asm volatile("atom.add.relaxed.sys.u32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(addend) : "memory")
#define __simt_add_release_32_device(ptr,old,addend) asm volatile("atom.add.release.gpu.u32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(addend) : "memory")
#define __simt_add_acquire_32_device(ptr,old,addend) asm volatile("atom.add.acquire.gpu.u32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(addend) : "memory")
#define __simt_add_acq_rel_32_device(ptr,old,addend) asm volatile("atom.add.acq_rel.gpu.u32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(addend) : "memory")
#define __simt_add_relaxed_32_device(ptr,old,addend) asm volatile("atom.add.relaxed.gpu.u32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(addend) : "memory")
#define __simt_add_release_32_block(ptr,old,addend) asm volatile("atom.add.release.cta.u32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(addend) : "memory")
#define __simt_add_acquire_32_block(ptr,old,addend) asm volatile("atom.add.acquire.cta.u32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(addend) : "memory")
#define __simt_add_acq_rel_32_block(ptr,old,addend) asm volatile("atom.add.acq_rel.cta.u32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(addend) : "memory")
#define __simt_add_relaxed_32_block(ptr,old,addend) asm volatile("atom.add.relaxed.cta.u32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(addend) : "memory")

#define __simt_and_release_32_system(ptr,old,andend) asm volatile("atom.and.release.sys.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(andend) : "memory")
#define __simt_and_acquire_32_system(ptr,old,andend) asm volatile("atom.and.acquire.sys.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(andend) : "memory")
#define __simt_and_acq_rel_32_system(ptr,old,andend) asm volatile("atom.and.acq_rel.sys.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(andend) : "memory")
#define __simt_and_relaxed_32_system(ptr,old,andend) asm volatile("atom.and.relaxed.sys.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(andend) : "memory")
#define __simt_and_release_32_device(ptr,old,andend) asm volatile("atom.and.release.gpu.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(andend) : "memory")
#define __simt_and_acquire_32_device(ptr,old,andend) asm volatile("atom.and.acquire.gpu.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(andend) : "memory")
#define __simt_and_acq_rel_32_device(ptr,old,andend) asm volatile("atom.and.acq_rel.gpu.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(andend) : "memory")
#define __simt_and_relaxed_32_device(ptr,old,andend) asm volatile("atom.and.relaxed.gpu.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(andend) : "memory")
#define __simt_and_release_32_block(ptr,old,andend) asm volatile("atom.and.release.cta.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(andend) : "memory")
#define __simt_and_acquire_32_block(ptr,old,andend) asm volatile("atom.and.acquire.cta.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(andend) : "memory")
#define __simt_and_acq_rel_32_block(ptr,old,andend) asm volatile("atom.and.acq_rel.cta.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(andend) : "memory")
#define __simt_and_relaxed_32_block(ptr,old,andend) asm volatile("atom.and.relaxed.cta.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(andend) : "memory")

#define __simt_or_release_32_system(ptr,old,orend) asm volatile("atom.or.release.sys.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(orend) : "memory")
#define __simt_or_acquire_32_system(ptr,old,orend) asm volatile("atom.or.acquire.sys.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(orend) : "memory")
#define __simt_or_acq_rel_32_system(ptr,old,orend) asm volatile("atom.or.acq_rel.sys.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(orend) : "memory")
#define __simt_or_relaxed_32_system(ptr,old,orend) asm volatile("atom.or.relaxed.sys.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(orend) : "memory")
#define __simt_or_release_32_device(ptr,old,orend) asm volatile("atom.or.release.gpu.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(orend) : "memory")
#define __simt_or_acquire_32_device(ptr,old,orend) asm volatile("atom.or.acquire.gpu.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(orend) : "memory")
#define __simt_or_acq_rel_32_device(ptr,old,orend) asm volatile("atom.or.acq_rel.gpu.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(orend) : "memory")
#define __simt_or_relaxed_32_device(ptr,old,orend) asm volatile("atom.or.relaxed.gpu.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(orend) : "memory")
#define __simt_or_release_32_block(ptr,old,orend) asm volatile("atom.or.release.cta.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(orend) : "memory")
#define __simt_or_acquire_32_block(ptr,old,orend) asm volatile("atom.or.acquire.cta.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(orend) : "memory")
#define __simt_or_acq_rel_32_block(ptr,old,orend) asm volatile("atom.or.acq_rel.cta.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(orend) : "memory")
#define __simt_or_relaxed_32_block(ptr,old,orend) asm volatile("atom.or.relaxed.cta.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(orend) : "memory")

#define __simt_xor_release_32_system(ptr,old,xorend) asm volatile("atom.xor.release.sys.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(xorend) : "memory")
#define __simt_xor_acquire_32_system(ptr,old,xorend) asm volatile("atom.xor.acquire.sys.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(xorend) : "memory")
#define __simt_xor_acq_rel_32_system(ptr,old,xorend) asm volatile("atom.xor.acq_rel.sys.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(xorend) : "memory")
#define __simt_xor_relaxed_32_system(ptr,old,xorend) asm volatile("atom.xor.relaxed.sys.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(xorend) : "memory")
#define __simt_xor_release_32_device(ptr,old,xorend) asm volatile("atom.xor.release.gpu.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(xorend) : "memory")
#define __simt_xor_acquire_32_device(ptr,old,xorend) asm volatile("atom.xor.acquire.gpu.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(xorend) : "memory")
#define __simt_xor_acq_rel_32_device(ptr,old,xorend) asm volatile("atom.xor.acq_rel.gpu.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(xorend) : "memory")
#define __simt_xor_relaxed_32_device(ptr,old,xorend) asm volatile("atom.xor.relaxed.gpu.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(xorend) : "memory")
#define __simt_xor_release_32_block(ptr,old,xorend) asm volatile("atom.xor.release.cta.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(xorend) : "memory")
#define __simt_xor_acquire_32_block(ptr,old,xorend) asm volatile("atom.xor.acquire.cta.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(xorend) : "memory")
#define __simt_xor_acq_rel_32_block(ptr,old,xorend) asm volatile("atom.xor.acq_rel.cta.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(xorend) : "memory")
#define __simt_xor_relaxed_32_block(ptr,old,xorend) asm volatile("atom.xor.relaxed.cta.b32 %0, [%1], %2;" : "=r"(old) : "l"(ptr), "r"(xorend) : "memory")


#define __simt_load_acquire_64_system(ptr,ret) asm volatile("ld.acquire.sys.b64 %0, [%1];" : "=l"(ret) : "l"(ptr) : "memory")
#define __simt_load_relaxed_64_system(ptr,ret) asm volatile("ld.relaxed.sys.b64 %0, [%1];" : "=l"(ret) : "l"(ptr) : "memory")
#define __simt_load_acquire_64_device(ptr,ret) asm volatile("ld.acquire.gpu.b64 %0, [%1];" : "=l"(ret) : "l"(ptr) : "memory")
#define __simt_load_relaxed_64_device(ptr,ret) asm volatile("ld.relaxed.gpu.b64 %0, [%1];" : "=l"(ret) : "l"(ptr) : "memory")
#define __simt_load_acquire_64_block(ptr,ret) asm volatile("ld.acquire.cta.b64 %0, [%1];" : "=l"(ret) : "l"(ptr) : "memory")
#define __simt_load_relaxed_64_block(ptr,ret) asm volatile("ld.relaxed.cta.b64 %0, [%1];" : "=l"(ret) : "l"(ptr) : "memory")

#define __simt_store_release_64_system(ptr,desired) asm volatile("st.release.sys.b64 [%0], %1;" :: "l"(ptr), "l"(desired) : "memory")
#define __simt_store_relaxed_64_system(ptr,desired) asm volatile("st.relaxed.sys.b64 [%0], %1;" :: "l"(ptr), "l"(desired) : "memory")
#define __simt_store_release_64_device(ptr,desired) asm volatile("st.release.gpu.b64 [%0], %1;" :: "l"(ptr), "l"(desired) : "memory")
#define __simt_store_relaxed_64_device(ptr,desired) asm volatile("st.relaxed.gpu.b64 [%0], %1;" :: "l"(ptr), "l"(desired) : "memory")
#define __simt_store_release_64_block(ptr,desired) asm volatile("st.release.cta.b64 [%0], %1;" :: "l"(ptr), "l"(desired) : "memory")
#define __simt_store_relaxed_64_block(ptr,desired) asm volatile("st.relaxed.cta.b64 [%0], %1;" :: "l"(ptr), "l"(desired) : "memory")

#define __simt_exch_release_64_system(ptr,old,desired) asm volatile("atom.exch.release.sys.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(desired) : "memory")
#define __simt_exch_acquire_64_system(ptr,old,desired) asm volatile("atom.exch.acquire.sys.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(desired) : "memory")
#define __simt_exch_acq_rel_64_system(ptr,old,desired) asm volatile("atom.exch.acq_rel.sys.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(desired) : "memory")
#define __simt_exch_relaxed_64_system(ptr,old,desired) asm volatile("atom.exch.relaxed.sys.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(desired) : "memory")
#define __simt_exch_release_64_device(ptr,old,desired) asm volatile("atom.exch.release.gpu.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(desired) : "memory")
#define __simt_exch_acquire_64_device(ptr,old,desired) asm volatile("atom.exch.acquire.gpu.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(desired) : "memory")
#define __simt_exch_acq_rel_64_device(ptr,old,desired) asm volatile("atom.exch.acq_rel.gpu.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(desired) : "memory")
#define __simt_exch_relaxed_64_device(ptr,old,desired) asm volatile("atom.exch.relaxed.gpu.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(desired) : "memory")
#define __simt_exch_release_64_block(ptr,old,desired) asm volatile("atom.exch.release.cta.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(desired) : "memory")
#define __simt_exch_acquire_64_block(ptr,old,desired) asm volatile("atom.exch.acquire.cta.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(desired) : "memory")
#define __simt_exch_acq_rel_64_block(ptr,old,desired) asm volatile("atom.exch.acq_rel.cta.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(desired) : "memory")
#define __simt_exch_relaxed_64_block(ptr,old,desired) asm volatile("atom.exch.relaxed.cta.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(desired) : "memory")

#define __simt_cas_release_64_system(ptr,old,expected,desired) asm volatile("atom.cas.release.sys.b64 %0, [%1], %2, %3;" : "=l"(old) : "l"(ptr), "l"(expected), "l"(desired) : "memory")
#define __simt_cas_acquire_64_system(ptr,old,expected,desired) asm volatile("atom.cas.acquire.sys.b64 %0, [%1], %2, %3;" : "=l"(old) : "l"(ptr), "l"(expected), "l"(desired) : "memory")
#define __simt_cas_acq_rel_64_system(ptr,old,expected,desired) asm volatile("atom.cas.acq_rel.sys.b64 %0, [%1], %2, %3;" : "=l"(old) : "l"(ptr), "l"(expected), "l"(desired) : "memory")
#define __simt_cas_relaxed_64_system(ptr,old,expected,desired) asm volatile("atom.cas.relaxed.sys.b64 %0, [%1], %2, %3;" : "=l"(old) : "l"(ptr), "l"(expected), "l"(desired) : "memory")
#define __simt_cas_release_64_device(ptr,old,expected,desired) asm volatile("atom.cas.release.gpu.b64 %0, [%1], %2, %3;" : "=l"(old) : "l"(ptr), "l"(expected), "l"(desired) : "memory")
#define __simt_cas_acquire_64_device(ptr,old,expected,desired) asm volatile("atom.cas.acquire.gpu.b64 %0, [%1], %2, %3;" : "=l"(old) : "l"(ptr), "l"(expected), "l"(desired) : "memory")
#define __simt_cas_acq_rel_64_device(ptr,old,expected,desired) asm volatile("atom.cas.acq_rel.gpu.b64 %0, [%1], %2, %3;" : "=l"(old) : "l"(ptr), "l"(expected), "l"(desired) : "memory")
#define __simt_cas_relaxed_64_device(ptr,old,expected,desired) asm volatile("atom.cas.relaxed.gpu.b64 %0, [%1], %2, %3;" : "=l"(old) : "l"(ptr), "l"(expected), "l"(desired) : "memory")
#define __simt_cas_release_64_block(ptr,old,expected,desired) asm volatile("atom.cas.release.cta.b64 %0, [%1], %2, %3;" : "=l"(old) : "l"(ptr), "l"(expected), "l"(desired) : "memory")
#define __simt_cas_acquire_64_block(ptr,old,expected,desired) asm volatile("atom.cas.acquire.cta.b64 %0, [%1], %2, %3;" : "=l"(old) : "l"(ptr), "l"(expected), "l"(desired) : "memory")
#define __simt_cas_acq_rel_64_block(ptr,old,expected,desired) asm volatile("atom.cas.acq_rel.cta.b64 %0, [%1], %2, %3;" : "=l"(old) : "l"(ptr), "l"(expected), "l"(desired) : "memory")
#define __simt_cas_relaxed_64_block(ptr,old,expected,desired) asm volatile("atom.cas.relaxed.cta.b64 %0, [%1], %2, %3;" : "=l"(old) : "l"(ptr), "l"(expected), "l"(desired) : "memory")

#define __simt_add_release_64_system(ptr,old,addend) asm volatile("atom.add.release.sys.u64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(addend) : "memory")
#define __simt_add_acquire_64_system(ptr,old,addend) asm volatile("atom.add.acquire.sys.u64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(addend) : "memory")
#define __simt_add_acq_rel_64_system(ptr,old,addend) asm volatile("atom.add.acq_rel.sys.u64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(addend) : "memory")
#define __simt_add_relaxed_64_system(ptr,old,addend) asm volatile("atom.add.relaxed.sys.u64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(addend) : "memory")
#define __simt_add_release_64_device(ptr,old,addend) asm volatile("atom.add.release.gpu.u64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(addend) : "memory")
#define __simt_add_acquire_64_device(ptr,old,addend) asm volatile("atom.add.acquire.gpu.u64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(addend) : "memory")
#define __simt_add_acq_rel_64_device(ptr,old,addend) asm volatile("atom.add.acq_rel.gpu.u64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(addend) : "memory")
#define __simt_add_relaxed_64_device(ptr,old,addend) asm volatile("atom.add.relaxed.gpu.u64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(addend) : "memory")
#define __simt_add_release_64_block(ptr,old,addend) asm volatile("atom.add.release.cta.u64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(addend) : "memory")
#define __simt_add_acquire_64_block(ptr,old,addend) asm volatile("atom.add.acquire.cta.u64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(addend) : "memory")
#define __simt_add_acq_rel_64_block(ptr,old,addend) asm volatile("atom.add.acq_rel.cta.u64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(addend) : "memory")
#define __simt_add_relaxed_64_block(ptr,old,addend) asm volatile("atom.add.relaxed.cta.u64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(addend) : "memory")

#define __simt_and_release_64_system(ptr,old,andend) asm volatile("atom.and.release.sys.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(andend) : "memory")
#define __simt_and_acquire_64_system(ptr,old,andend) asm volatile("atom.and.acquire.sys.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(andend) : "memory")
#define __simt_and_acq_rel_64_system(ptr,old,andend) asm volatile("atom.and.acq_rel.sys.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(andend) : "memory")
#define __simt_and_relaxed_64_system(ptr,old,andend) asm volatile("atom.and.relaxed.sys.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(andend) : "memory")
#define __simt_and_release_64_device(ptr,old,andend) asm volatile("atom.and.release.gpu.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(andend) : "memory")
#define __simt_and_acquire_64_device(ptr,old,andend) asm volatile("atom.and.acquire.gpu.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(andend) : "memory")
#define __simt_and_acq_rel_64_device(ptr,old,andend) asm volatile("atom.and.acq_rel.gpu.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(andend) : "memory")
#define __simt_and_relaxed_64_device(ptr,old,andend) asm volatile("atom.and.relaxed.gpu.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(andend) : "memory")
#define __simt_and_release_64_block(ptr,old,andend) asm volatile("atom.and.release.cta.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(andend) : "memory")
#define __simt_and_acquire_64_block(ptr,old,andend) asm volatile("atom.and.acquire.cta.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(andend) : "memory")
#define __simt_and_acq_rel_64_block(ptr,old,andend) asm volatile("atom.and.acq_rel.cta.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(andend) : "memory")
#define __simt_and_relaxed_64_block(ptr,old,andend) asm volatile("atom.and.relaxed.cta.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(andend) : "memory")

#define __simt_or_release_64_system(ptr,old,orend) asm volatile("atom.or.release.sys.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(orend) : "memory")
#define __simt_or_acquire_64_system(ptr,old,orend) asm volatile("atom.or.acquire.sys.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(orend) : "memory")
#define __simt_or_acq_rel_64_system(ptr,old,orend) asm volatile("atom.or.acq_rel.sys.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(orend) : "memory")
#define __simt_or_relaxed_64_system(ptr,old,orend) asm volatile("atom.or.relaxed.sys.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(orend) : "memory")
#define __simt_or_release_64_device(ptr,old,orend) asm volatile("atom.or.release.gpu.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(orend) : "memory")
#define __simt_or_acquire_64_device(ptr,old,orend) asm volatile("atom.or.acquire.gpu.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(orend) : "memory")
#define __simt_or_acq_rel_64_device(ptr,old,orend) asm volatile("atom.or.acq_rel.gpu.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(orend) : "memory")
#define __simt_or_relaxed_64_device(ptr,old,orend) asm volatile("atom.or.relaxed.gpu.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(orend) : "memory")
#define __simt_or_release_64_block(ptr,old,orend) asm volatile("atom.or.release.cta.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(orend) : "memory")
#define __simt_or_acquire_64_block(ptr,old,orend) asm volatile("atom.or.acquire.cta.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(orend) : "memory")
#define __simt_or_acq_rel_64_block(ptr,old,orend) asm volatile("atom.or.acq_rel.cta.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(orend) : "memory")
#define __simt_or_relaxed_64_block(ptr,old,orend) asm volatile("atom.or.relaxed.cta.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(orend) : "memory")

#define __simt_xor_release_64_system(ptr,old,xorend) asm volatile("atom.xor.release.sys.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(xorend) : "memory")
#define __simt_xor_acquire_64_system(ptr,old,xorend) asm volatile("atom.xor.acquire.sys.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(xorend) : "memory")
#define __simt_xor_acq_rel_64_system(ptr,old,xorend) asm volatile("atom.xor.acq_rel.sys.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(xorend) : "memory")
#define __simt_xor_relaxed_64_system(ptr,old,xorend) asm volatile("atom.xor.relaxed.sys.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(xorend) : "memory")
#define __simt_xor_release_64_device(ptr,old,xorend) asm volatile("atom.xor.release.gpu.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(xorend) : "memory")
#define __simt_xor_acquire_64_device(ptr,old,xorend) asm volatile("atom.xor.acquire.gpu.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(xorend) : "memory")
#define __simt_xor_acq_rel_64_device(ptr,old,xorend) asm volatile("atom.xor.acq_rel.gpu.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(xorend) : "memory")
#define __simt_xor_relaxed_64_device(ptr,old,xorend) asm volatile("atom.xor.relaxed.gpu.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(xorend) : "memory")
#define __simt_xor_release_64_block(ptr,old,xorend) asm volatile("atom.xor.release.cta.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(xorend) : "memory")
#define __simt_xor_acquire_64_block(ptr,old,xorend) asm volatile("atom.xor.acquire.cta.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(xorend) : "memory")
#define __simt_xor_acq_rel_64_block(ptr,old,xorend) asm volatile("atom.xor.acq_rel.cta.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(xorend) : "memory")
#define __simt_xor_relaxed_64_block(ptr,old,xorend) asm volatile("atom.xor.relaxed.cta.b64 %0, [%1], %2;" : "=l"(old) : "l"(ptr), "l"(xorend) : "memory")


#define __simt_nanosleep(timeout) asm volatile("nanosleep.u32 %0;" :: "r"(unsigned(timeout)) : )

/*
    definitions
*/

#define __GCC_ATOMIC_BOOL_LOCK_FREE 2
#define __GCC_ATOMIC_CHAR_LOCK_FREE 2
#define __GCC_ATOMIC_CHAR16_T_LOCK_FREE 2
#define __GCC_ATOMIC_CHAR32_T_LOCK_FREE 2
#define __GCC_ATOMIC_WCHAR_T_LOCK_FREE 2
#define __GCC_ATOMIC_SHORT_LOCK_FREE 2
#define __GCC_ATOMIC_INT_LOCK_FREE 2
#define __GCC_ATOMIC_LONG_LOCK_FREE 2
#define __GCC_ATOMIC_LLONG_LOCK_FREE 2
#define __GCC_ATOMIC_POINTER_LOCK_FREE 2

#define __ATOMIC_RELAXED 0
#define __ATOMIC_CONSUME 1
#define __ATOMIC_ACQUIRE 2
#define __ATOMIC_RELEASE 3
#define __ATOMIC_ACQ_REL 4
#define __ATOMIC_SEQ_CST 5

inline __device__ int __stronger_order_simt_(int a, int b) {
    if (b == __ATOMIC_SEQ_CST) return __ATOMIC_SEQ_CST;
    if (b == __ATOMIC_RELAXED) return a;
    switch (a) {
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: return a;
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: if (b != __ATOMIC_ACQUIRE) return __ATOMIC_ACQ_REL; else return __ATOMIC_ACQUIRE;
    case __ATOMIC_RELEASE: if (b != __ATOMIC_RELEASE) return __ATOMIC_ACQ_REL; else return __ATOMIC_RELEASE;
    case __ATOMIC_RELAXED: return b;
    default: assert(0);
    }
    return __ATOMIC_SEQ_CST;
}

/*
    base
*/

#define DO__atomic_load_simt_(bytes, bits, scope) \
    template<class type, typename simt::std::enable_if<sizeof(type)==bytes, int>::type = 0> \
    void __device__ __atomic_load_simt_ (const type *ptr, type *ret, int memorder, __atomic_scope_##scope) { \
        int##bits##_t tmp = 0; \
        switch (memorder) { \
        case __ATOMIC_SEQ_CST: __simt_fence_sc_##scope##_(); \
        case __ATOMIC_CONSUME: \
        case __ATOMIC_ACQUIRE: __simt_load_acquire_##bits##_##scope(ptr, tmp); break; \
        case __ATOMIC_RELAXED: __simt_load_relaxed_##bits##_##scope(ptr, tmp); break; \
        default: assert(0); \
        } \
        memcpy(ret, &tmp, bytes); \
    }
DO__atomic_load_simt_(1,32,system)
DO__atomic_load_simt_(2,16,system)
DO__atomic_load_simt_(4,32,system)
DO__atomic_load_simt_(8,64,system)
DO__atomic_load_simt_(1,32,device)
DO__atomic_load_simt_(2,16,device)
DO__atomic_load_simt_(4,32,device)
DO__atomic_load_simt_(8,64,device)
DO__atomic_load_simt_(1,32,block)
DO__atomic_load_simt_(2,16,block)
DO__atomic_load_simt_(4,32,block)
DO__atomic_load_simt_(8,64,block)

template<class type, class scope>
type __device__ __atomic_load_n_simt_(const type *ptr, int memorder, scope s) {
    type ret;
    __atomic_load_simt_(ptr, &ret, memorder, s);
    return ret;
}

#define DO__atomic_store_simt_(bytes, bits, scope) \
template<class type, typename simt::std::enable_if<sizeof(type)==bytes, int>::type = 0> \
void __device__ __atomic_store_simt_ (type *ptr, type *val, int memorder, __atomic_scope_##scope) { \
    int##bits##_t tmp = 0; \
    memcpy(&tmp, val, bytes); \
    switch (memorder) { \
    case __ATOMIC_RELEASE: __simt_store_release_##bits##_##scope(ptr, tmp); break; \
    case __ATOMIC_SEQ_CST: __simt_fence_sc_##scope##_(); \
    case __ATOMIC_RELAXED: __simt_store_relaxed_##bits##_##scope(ptr, tmp); break; \
    default: assert(0); \
    } \
}
DO__atomic_store_simt_(1,32,system)
DO__atomic_store_simt_(2,16,system)
DO__atomic_store_simt_(4,32,system)
DO__atomic_store_simt_(8,64,system)
DO__atomic_store_simt_(1,32,device)
DO__atomic_store_simt_(2,16,device)
DO__atomic_store_simt_(4,32,device)
DO__atomic_store_simt_(8,64,device)
DO__atomic_store_simt_(1,32,block)
DO__atomic_store_simt_(2,16,block)
DO__atomic_store_simt_(4,32,block)
DO__atomic_store_simt_(8,64,block)

template<class type, class scope>
void __device__ __atomic_store_n_simt_(type *ptr, type val, int memorder, scope s) {
    __atomic_store_simt_(ptr, &val, memorder, s);
}

#define DO__atomic_compare_exchange_simt_(bytes, bits, scope) \
template<class type, typename simt::std::enable_if<sizeof(type)==bytes, int>::type = 0> \
bool __device__ __atomic_compare_exchange_simt_ (type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __atomic_scope_##scope) { \
    int##bits##_t tmp = 0, old = 0, old_tmp; \
    memcpy(&tmp, desired, bytes); \
    memcpy(&old, expected, bytes); \
    old_tmp = old; \
    switch (__stronger_order_simt_(success_memorder, failure_memorder)) { \
    case __ATOMIC_SEQ_CST: __simt_fence_sc_##scope##_(); \
    case __ATOMIC_CONSUME: \
    case __ATOMIC_ACQUIRE: __simt_cas_acquire_##bits##_##scope(ptr, old, old_tmp, tmp); break; \
    case __ATOMIC_ACQ_REL: __simt_cas_acq_rel_##bits##_##scope(ptr, old, old_tmp, tmp); break; \
    case __ATOMIC_RELEASE: __simt_cas_release_##bits##_##scope(ptr, old, old_tmp, tmp); break; \
    case __ATOMIC_RELAXED: __simt_cas_relaxed_##bits##_##scope(ptr, old, old_tmp, tmp); break; \
    default: assert(0); \
    } \
    bool const ret = old == old_tmp; \
    memcpy(expected, &old, bytes); \
    return ret; \
}
DO__atomic_compare_exchange_simt_(4,32,system)
DO__atomic_compare_exchange_simt_(8,64,system)
DO__atomic_compare_exchange_simt_(4,32,device)
DO__atomic_compare_exchange_simt_(8,64,device)
DO__atomic_compare_exchange_simt_(4,32,block)
DO__atomic_compare_exchange_simt_(8,64,block)

template<class type, class scope, typename simt::std::enable_if<sizeof(type) <= 2, int>::type = 0> \
bool __device__ __atomic_compare_exchange_simt_(type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, scope s) {

    using R = typename simt::std::conditional<simt::std::is_volatile<type>::value, volatile uint32_t, uint32_t>::type;
    auto const aligned = (R*)((intptr_t)ptr & ~(sizeof(uint32_t) - 1));
    auto const offset = uint32_t((intptr_t)ptr & (sizeof(uint32_t) - 1)) * 8;
    auto const mask = ((1 << sizeof(type)*8) - 1) << offset;

    uint32_t old = *expected << offset, old_value;
    while (1) {
        old_value = (old & mask) >> offset;
        if (old_value != *expected)
            break;
        uint32_t const attempt = (old & ~mask) | (*desired << offset);
        if (__atomic_compare_exchange_simt_ (aligned, &old, &attempt, true, success_memorder, failure_memorder, s))
            return true;
    }
    *expected = old_value;
    return false;
}

template<class type, class scope>
bool __device__ __atomic_compare_exchange_n_simt_(type *ptr, type *expected, type desired, bool weak, int success_memorder, int failure_memorder, scope s) {
    return __atomic_compare_exchange_simt_(ptr, expected, &desired, weak, success_memorder, failure_memorder, s);
}

#define DO__atomic_exchange_simt_(bytes, bits, scope) \
template<class type, typename simt::std::enable_if<sizeof(type)==bytes, int>::type = 0> \
void __device__ __atomic_exchange_simt_ (type *ptr, type *val, type *ret, int memorder, __atomic_scope_##scope) { \
    int##bits##_t tmp = 0; \
    memcpy(&tmp, val, bytes); \
    switch (memorder) { \
    case __ATOMIC_SEQ_CST: __simt_fence_sc_##scope##_(); \
    case __ATOMIC_CONSUME: \
    case __ATOMIC_ACQUIRE: __simt_exch_acquire_##bits##_##scope(ptr, tmp, tmp); break; \
    case __ATOMIC_ACQ_REL: __simt_exch_acq_rel_##bits##_##scope(ptr, tmp, tmp); break; \
    case __ATOMIC_RELEASE: __simt_exch_release_##bits##_##scope(ptr, tmp, tmp); break; \
    case __ATOMIC_RELAXED: __simt_exch_relaxed_##bits##_##scope(ptr, tmp, tmp); break; \
    default: assert(0); \
    } \
    memcpy(ret, &tmp, bytes); \
}
DO__atomic_exchange_simt_(4,32,system)
DO__atomic_exchange_simt_(8,64,system)
DO__atomic_exchange_simt_(4,32,device)
DO__atomic_exchange_simt_(8,64,device)
DO__atomic_exchange_simt_(4,32,block)
DO__atomic_exchange_simt_(8,64,block)

template<class type, class scope, typename simt::std::enable_if<sizeof(type)<=2, int>::type = 0>
void __device__ __atomic_exchange_simt_ (type *ptr, type *val, type *ret, int memorder, scope s) {

    type expected = __atomic_load_n_simt_(ptr, __ATOMIC_RELAXED, s);
    while(!__atomic_compare_exchange_simt_(ptr, &expected, val, true, memorder, memorder, s))
        ;
    *ret = expected;
}

template<class type, class scope>
type __device__ __atomic_exchange_n_simt_(type *ptr, type val, int memorder, scope s) {
    type ret;
    __atomic_exchange_simt_(ptr, &val, &ret, memorder, s);
    return ret;
}

#define DO__atomic_fetch_add_simt_(bytes, bits, scope) \
template<class type, class delta, typename simt::std::enable_if<sizeof(type)==bytes, int>::type = 0> \
type __device__ __atomic_fetch_add_simt_ (type *ptr, delta val, int memorder, __atomic_scope_##scope) { \
    type ret; \
    switch (memorder) { \
    case __ATOMIC_SEQ_CST: __simt_fence_sc_##scope##_(); \
    case __ATOMIC_CONSUME: \
    case __ATOMIC_ACQUIRE: __simt_add_acquire_##bits##_##scope(ptr, ret, val); break; \
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_##bits##_##scope(ptr, ret, val); break; \
    case __ATOMIC_RELEASE: __simt_add_release_##bits##_##scope(ptr, ret, val); break; \
    case __ATOMIC_RELAXED: __simt_add_relaxed_##bits##_##scope(ptr, ret, val); break; \
    default: assert(0); \
    } \
    return ret; \
}
DO__atomic_fetch_add_simt_(4,32,system)
DO__atomic_fetch_add_simt_(8,64,system)
DO__atomic_fetch_add_simt_(4,32,device)
DO__atomic_fetch_add_simt_(8,64,device)
DO__atomic_fetch_add_simt_(4,32,block)
DO__atomic_fetch_add_simt_(8,64,block)

template<class type, class delta, class scope, typename simt::std::enable_if<sizeof(type)<=2, int>::type = 0>
type __device__ __atomic_fetch_add_simt_ (type *ptr, delta val, int memorder, scope s) {

    type expected = __atomic_load_n_simt_(ptr, __ATOMIC_RELAXED, s);
    type const desired = expected + val;
    while(!__atomic_compare_exchange_simt_(ptr, &expected, &desired, true, memorder, memorder, s))
        ;
    return expected;
}

#define DO__atomic_fetch_sub_simt_(bytes, bits, scope) \
template<class type, class delta, typename simt::std::enable_if<sizeof(type)==bytes, int>::type = 0> \
type __device__ __atomic_fetch_sub_simt_ (type *ptr, delta val, int memorder, __atomic_scope_##scope) { \
    type ret; \
    switch (memorder) { \
    case __ATOMIC_SEQ_CST: __simt_fence_sc_##scope##_(); \
    case __ATOMIC_CONSUME: \
    case __ATOMIC_ACQUIRE: __simt_add_acquire_##bits##_##scope(ptr, ret, -val); break; \
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_##bits##_##scope(ptr, ret, -val); break; \
    case __ATOMIC_RELEASE: __simt_add_release_##bits##_##scope(ptr, ret, -val); break; \
    case __ATOMIC_RELAXED: __simt_add_relaxed_##bits##_##scope(ptr, ret, -val); break; \
    default: assert(0); \
    } \
    return ret; \
}
DO__atomic_fetch_sub_simt_(4,32,system)
DO__atomic_fetch_sub_simt_(8,64,system)
DO__atomic_fetch_sub_simt_(4,32,device)
DO__atomic_fetch_sub_simt_(8,64,device)
DO__atomic_fetch_sub_simt_(4,32,block)
DO__atomic_fetch_sub_simt_(8,64,block)

template<class type, class delta, class scope, typename simt::std::enable_if<sizeof(type)<=2, int>::type = 0>
type __device__ __atomic_fetch_sub_simt_ (type *ptr, delta val, int memorder, scope s) {

    type expected = __atomic_load_n_simt_(ptr, __ATOMIC_RELAXED, s);
    type const desired = expected - val;
    while(!__atomic_compare_exchange_simt_(ptr, &expected, &desired, true, memorder, memorder, s))
        ;
    return expected;
}

#define DO__atomic_fetch_and_simt_(bytes, bits, scope) \
template<class type, typename simt::std::enable_if<sizeof(type)==bytes, int>::type = 0> \
type __device__ __atomic_fetch_and_simt_ (type *ptr, type val, int memorder, __atomic_scope_##scope) { \
    type ret; \
    switch (memorder) { \
    case __ATOMIC_SEQ_CST: __simt_fence_sc_##scope##_(); \
    case __ATOMIC_CONSUME: \
    case __ATOMIC_ACQUIRE: __simt_and_acquire_##bits##_##scope(ptr, ret, val); break; \
    case __ATOMIC_ACQ_REL: __simt_and_acq_rel_##bits##_##scope(ptr, ret, val); break; \
    case __ATOMIC_RELEASE: __simt_and_release_##bits##_##scope(ptr, ret, val); break; \
    case __ATOMIC_RELAXED: __simt_and_relaxed_##bits##_##scope(ptr, ret, val); break; \
    default: assert(0); \
    } \
    return ret; \
}
DO__atomic_fetch_and_simt_(4,32,system)
DO__atomic_fetch_and_simt_(8,64,system)
DO__atomic_fetch_and_simt_(4,32,device)
DO__atomic_fetch_and_simt_(8,64,device)
DO__atomic_fetch_and_simt_(4,32,block)
DO__atomic_fetch_and_simt_(8,64,block)

template<class type, class delta, class scope, typename simt::std::enable_if<sizeof(type)<=2, int>::type = 0>
type __device__ __atomic_fetch_and_simt_ (type *ptr, delta val, int memorder, scope s) {

    type expected = __atomic_load_n_simt_(ptr, __ATOMIC_RELAXED, s);
    type const desired = expected & val;
    while(!__atomic_compare_exchange_simt_(ptr, &expected, &desired, true, memorder, memorder, s))
        ;
    return expected;
}

#define DO__atomic_fetch_xor_simt_(bytes, bits, scope) \
template<class type, typename simt::std::enable_if<sizeof(type)==bytes, int>::type = 0> \
type __device__ __atomic_fetch_xor_simt_ (type *ptr, type val, int memorder, __atomic_scope_##scope) { \
    type ret; \
    switch (memorder) { \
    case __ATOMIC_SEQ_CST: __simt_fence_sc_##scope##_(); \
    case __ATOMIC_CONSUME: \
    case __ATOMIC_ACQUIRE: __simt_xor_acquire_##bits##_##scope(ptr, ret, val); break; \
    case __ATOMIC_ACQ_REL: __simt_xor_acq_rel_##bits##_##scope(ptr, ret, val); break; \
    case __ATOMIC_RELEASE: __simt_xor_release_##bits##_##scope(ptr, ret, val); break; \
    case __ATOMIC_RELAXED: __simt_xor_relaxed_##bits##_##scope(ptr, ret, val); break; \
    default: assert(0); \
    } \
    return ret; \
}
DO__atomic_fetch_xor_simt_(4,32,system)
DO__atomic_fetch_xor_simt_(8,64,system)
DO__atomic_fetch_xor_simt_(4,32,device)
DO__atomic_fetch_xor_simt_(8,64,device)
DO__atomic_fetch_xor_simt_(4,32,block)
DO__atomic_fetch_xor_simt_(8,64,block)

template<class type, class delta, class scope, typename simt::std::enable_if<sizeof(type)<=2, int>::type = 0>
type __device__ __atomic_fetch_xor_simt_ (type *ptr, delta val, int memorder, scope s) {

    type expected = __atomic_load_n_simt_(ptr, __ATOMIC_RELAXED, s);
    type const desired = expected ^ val;
    while(!__atomic_compare_exchange_simt_(ptr, &expected, &desired, true, memorder, memorder, s))
        ;
    return expected;
}

#define DO__atomic_fetch_or_simt_(bytes, bits, scope) \
template<class type, typename simt::std::enable_if<sizeof(type)==bytes, int>::type = 0> \
type __device__ __atomic_fetch_or_simt_ (type *ptr, type val, int memorder, __atomic_scope_##scope) { \
    type ret; \
    switch (memorder) { \
    case __ATOMIC_SEQ_CST: __simt_fence_sc_##scope##_(); \
    case __ATOMIC_CONSUME: \
    case __ATOMIC_ACQUIRE: __simt_or_acquire_##bits##_##scope(ptr, ret, val); break; \
    case __ATOMIC_ACQ_REL: __simt_or_acq_rel_##bits##_##scope(ptr, ret, val); break; \
    case __ATOMIC_RELEASE: __simt_or_release_##bits##_##scope(ptr, ret, val); break; \
    case __ATOMIC_RELAXED: __simt_or_relaxed_##bits##_##scope(ptr, ret, val); break; \
    default: assert(0); \
    } \
    return ret; \
}
DO__atomic_fetch_or_simt_(4,32,system)
DO__atomic_fetch_or_simt_(8,64,system)
DO__atomic_fetch_or_simt_(4,32,device)
DO__atomic_fetch_or_simt_(8,64,device)
DO__atomic_fetch_or_simt_(4,32,block)
DO__atomic_fetch_or_simt_(8,64,block)

template<class type, class delta, class scope, typename simt::std::enable_if<sizeof(type)<=2, int>::type = 0>
type __device__ __atomic_fetch_or_simt_ (type *ptr, delta val, int memorder, scope s) {

    type expected = __atomic_load_n_simt_(ptr, __ATOMIC_RELAXED, s);
    type const desired = expected | val;
    while(!__atomic_compare_exchange_simt_(ptr, &expected, &desired, true, memorder, memorder, s))
        ;
    return expected;
}

template<class type, class scope>
inline bool __device__ __atomic_test_and_set_simt_(type *ptr, int memorder, scope s) {
    return __atomic_exchange_n_simt_((char*)ptr, (char)1, memorder, s) == 1;
}
template<class type, class scope>
inline void __device__ __atomic_clear_simt_(type *ptr, int memorder, scope s) {
    return __atomic_store_n_simt_((char*)ptr, (char)0, memorder, s);
}

inline constexpr __device__ bool __atomic_always_lock_free_simt_ (size_t size, void *) {
    return size <= 8;
}
inline __device__ bool __atomic_is_lock_free_simt_(size_t size, void * ptr) {
    return __atomic_always_lock_free_simt_(size, ptr);
}

/*
    fences
*/
#define DO__atomic_thread_fence_simt_(scope) \
    inline void __device__ __atomic_thread_fence_simt(int memorder, __atomic_scope_##scope) { \
        switch (memorder) { \
        case __ATOMIC_SEQ_CST: __simt_fence_sc_##scope##_(); break; \
        case __ATOMIC_CONSUME: \
        case __ATOMIC_ACQUIRE: \
        case __ATOMIC_ACQ_REL: \
        case __ATOMIC_RELEASE: __simt_fence_##scope##_(); break; \
        case __ATOMIC_RELAXED: break; \
        default: assert(0); \
        } \
    }
DO__atomic_thread_fence_simt_(system)
DO__atomic_thread_fence_simt_(device)
DO__atomic_thread_fence_simt_(block)

inline void __device__ __atomic_signal_fence_simt(int memorder) { 
    __simt_fence_signal_();
}

/*
    non-volatile
*/

template<class type, class scope> type __device__ __atomic_load_n_simt(const type *ptr, int memorder, scope s) {
    return __atomic_load_n_simt_(const_cast<const type*>(ptr), memorder, s);
}
template<class type, class scope> void __device__ __atomic_load_simt(const type *ptr, type *ret, int memorder, scope s) {
    __atomic_load_simt_(const_cast<const type*>(ptr), ret, memorder, s);
}
template<class type, class scope> void __device__ __atomic_store_n_simt(type *ptr, type val, int memorder, scope s) {
    __atomic_store_n_simt_(const_cast<type*>(ptr), val, memorder, s);
}
template<class type, class scope> void __device__ __atomic_store_simt(type *ptr, type *val, int memorder, scope s) {
    __atomic_store_simt_(const_cast<type*>(ptr), val, memorder, s);
}
template<class type, class scope> type __device__ __atomic_exchange_n_simt(type *ptr, type val, int memorder, scope s) {
    return __atomic_exchange_n_simt_(const_cast<type*>(ptr), val, memorder, s);
}
template<class type, class scope> void __device__ __atomic_exchange_simt(type *ptr, type *val, type *ret, int memorder, scope s) {
    __atomic_exchange_simt_(const_cast<type*>(ptr), val, ret, memorder, s);
}
template<class type, class scope> bool __device__ __atomic_compare_exchange_n_simt(type *ptr, type *expected, type desired, bool weak, int success_memorder, int failure_memorder, scope s) {
    return __atomic_compare_exchange_n_simt_(const_cast<type*>(ptr), expected, desired, weak, success_memorder, failure_memorder, s);
}
template<class type, class scope> bool __device__ __atomic_compare_exchange_simt(type *ptr, type *expected, type *desired, bool weak, int success_memorder, int failure_memorder, scope s) {
    return __atomic_compare_exchange_simt_(const_cast<type*>(ptr), expected, desired, weak, success_memorder, failure_memorder, s);
}
template<class type, class delta, class scope> type __device__ __atomic_fetch_add_simt(type *ptr, delta val, int memorder, scope s) {
    return __atomic_fetch_add_simt_(const_cast<type*>(ptr), val, memorder, s);
}
template<class type, class delta, class scope> type __device__ __atomic_fetch_sub_simt(type *ptr, delta val, int memorder, scope s) {
    return __atomic_fetch_sub_simt_(const_cast<type*>(ptr), val, memorder, s);
}
template<class type, class scope> type __device__ __atomic_fetch_and_simt(type *ptr, type val, int memorder, scope s) {
    return __atomic_fetch_and_simt_(const_cast<type*>(ptr), val, memorder, s);
}
template<class type, class scope> type __device__ __atomic_fetch_xor_simt(type *ptr, type val, int memorder, scope s) {
    return __atomic_fetch_xor_simt_(const_cast<type*>(ptr), val, memorder, s);
}
template<class type, class scope> type __device__ __atomic_fetch_or_simt(type *ptr, type val, int memorder, scope s) {
    return __atomic_fetch_or_simt_(const_cast<type*>(ptr), val, memorder, s);
}
template<class type, class scope> bool __device__ __atomic_test_and_set_simt(void *ptr, int memorder, scope s) {
    return __atomic_test_and_set_simt_(const_cast<void*>(ptr), memorder, s);
}
template<class type, class scope> void __device__ __atomic_clear_simt(void *ptr, int memorder, scope s) {
    return __atomic_clear_simt_(const_cast<void*>(ptr), memorder, s);
}
inline bool __device__ __atomic_always_lock_free_simt(size_t size, void *ptr) {
    return __atomic_always_lock_free_simt_(size, const_cast<void*>(ptr));
}
inline bool __device__ __atomic_is_lock_free_simt(size_t size, void *ptr) {
    return __atomic_is_lock_free_simt_(size, const_cast<void*>(ptr));
}

/*
    volatile 
*/

template<class type, class scope> type __device__ __atomic_load_n_simt(const volatile type *ptr, int memorder, scope s) {
    return __atomic_load_n_simt_(const_cast<const type*>(ptr), memorder, s);
}
template<class type, class scope> void __device__ __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, scope s) {
    __atomic_load_simt_(const_cast<const type*>(ptr), ret, memorder, s);
}
template<class type, class scope> void __device__ __atomic_store_n_simt(volatile type *ptr, type val, int memorder, scope s) {
    __atomic_store_n_simt_(const_cast<type*>(ptr), val, memorder, s);
}
template<class type, class scope> void __device__ __atomic_store_simt(volatile type *ptr, type *val, int memorder, scope s) {
    __atomic_store_simt_(const_cast<type*>(ptr), val, memorder, s);
}
template<class type, class scope> type __device__ __atomic_exchange_n_simt(volatile type *ptr, type val, int memorder, scope s) {
    return __atomic_exchange_n_simt_(const_cast<type*>(ptr), val, memorder, s);
}
template<class type, class scope> void __device__ __atomic_exchange_simt(volatile type *ptr, type *val, type *ret, int memorder, scope s) {
    __atomic_exchange_simt_(const_cast<type*>(ptr), val, ret, memorder, s);
}
template<class type, class scope> bool __device__ __atomic_compare_exchange_n_simt(volatile type *ptr, type *expected, type desired, bool weak, int success_memorder, int failure_memorder, scope s) {
    return __atomic_compare_exchange_n_simt_(const_cast<type*>(ptr), expected, desired, weak, success_memorder, failure_memorder, s);
}
template<class type, class scope> bool __device__ __atomic_compare_exchange_simt(volatile type *ptr, type *expected, type *desired, bool weak, int success_memorder, int failure_memorder, scope s) {
    return __atomic_compare_exchange_simt_(const_cast<type*>(ptr), expected, desired, weak, success_memorder, failure_memorder, s);
}
template<class type, class delta, class scope> type __device__ __atomic_fetch_add_simt(volatile type *ptr, delta val, int memorder, scope s) {
    return __atomic_fetch_add_simt_(const_cast<type*>(ptr), val, memorder, s);
}
template<class type, class delta, class scope> type __device__ __atomic_fetch_sub_simt(volatile type *ptr, delta val, int memorder, scope s) {
    return __atomic_fetch_sub_simt_(const_cast<type*>(ptr), val, memorder, s);
}
template<class type, class scope> type __device__ __atomic_fetch_and_simt(volatile type *ptr, type val, int memorder, scope s) {
    return __atomic_fetch_and_simt_(const_cast<type*>(ptr), val, memorder, s);
}
template<class type, class scope> type __device__ __atomic_fetch_xor_simt(volatile type *ptr, type val, int memorder, scope s) {
    return __atomic_fetch_xor_simt_(const_cast<type*>(ptr), val, memorder, s);
}
template<class type, class scope> type __device__ __atomic_fetch_or_simt(volatile type *ptr, type val, int memorder, scope s) {
    return __atomic_fetch_or_simt_(const_cast<type*>(ptr), val, memorder, s);
}
template<class type, class scope> bool __device__ __atomic_test_and_set_simt(volatile void *ptr, int memorder, scope s) {
    return __atomic_test_and_set_simt_(const_cast<void*>(ptr), memorder, s);
}
template<class type, class scope> void __device__ __atomic_clear_simt(volatile void *ptr, int memorder, scope s) {
    return __atomic_clear_simt_(const_cast<void*>(ptr), memorder, s);
}
inline bool __device__ __atomic_always_lock_free_simt(size_t size, volatile void *ptr) {
    return __atomic_always_lock_free_simt_(size, const_cast<void*>(ptr));
}
inline bool __device__ __atomic_is_lock_free_simt(size_t size, volatile void *ptr) {
    return __atomic_is_lock_free_simt_(size, const_cast<void*>(ptr));
}

/*
    builtins
*/

#define __atomic_load_n __atomic_load_n_simt
#define __atomic_load __atomic_load_simt
#define __atomic_store_n __atomic_store_n_simt
#define __atomic_store __atomic_store_simt
#define __atomic_exchange_n __atomic_exchange_n_simt
#define __atomic_exchange __atomic_exchange_simt
#define __atomic_compare_exchange_n __atomic_compare_exchange_n_simt
#define __atomic_compare_exchange __atomic_compare_exchange_simt
#define __atomic_fetch_add __atomic_fetch_add_simt
#define __atomic_fetch_sub __atomic_fetch_sub_simt
#define __atomic_fetch_and __atomic_fetch_and_simt
#define __atomic_fetch_xor __atomic_fetch_xor_simt
#define __atomic_fetch_or __atomic_fetch_or_simt
#define __atomic_test_and_set __atomic_test_and_set_simt
#define __atomic_clear __atomic_clear_simt
#define __atomic_always_lock_free __atomic_always_lock_free_simt
#define __atomic_is_lock_free __atomic_is_lock_free_simt
#define __atomic_thread_fence __atomic_thread_fence_simt
#define __atomic_signal_fence __atomic_signal_fence_simt

#endif //__CUDA_ARCH__

#endif //_SIMT_DETAILS_CONFIG
