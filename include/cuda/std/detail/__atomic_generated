/*

Copyright (c) 2018-2019, NVIDIA Corporation

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

*/

_LIBCPP_BEGIN_NAMESPACE_CUDA
namespace detail {

static inline __device__ void __cuda_fence_sc_block() { asm volatile("fence.sc.cta;":::"memory"); }
static inline __device__ void __cuda_fence_acq_rel_block() { asm volatile("fence.acq_rel.cta;":::"memory"); }
static inline __device__ void __atomic_thread_fence_cuda(int memorder, __thread_scope_block_tag) {
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block(); break;
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE:
    case __ATOMIC_ACQ_REL:
    case __ATOMIC_RELEASE: __cuda_fence_acq_rel_block(); break;
    case __ATOMIC_RELAXED: break;
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_relaxed_8_block(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.relaxed.cta.b8 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_acquire_8_block(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.acquire.cta.b8 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename _VSTD::enable_if<sizeof(type)==1, int>::type = 0>
__device__ void __atomic_load_cuda(const volatile type *ptr, type *ret, int memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_acquire_8_block(ptr, tmp); break;
    case __ATOMIC_RELAXED: __cuda_load_relaxed_8_block(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 1);
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_relaxed_16_block(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.relaxed.cta.b16 %0,[%1];" : "=h"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_acquire_16_block(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.acquire.cta.b16 %0,[%1];" : "=h"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename _VSTD::enable_if<sizeof(type)==2, int>::type = 0>
__device__ void __atomic_load_cuda(const volatile type *ptr, type *ret, int memorder, __thread_scope_block_tag) {
    uint16_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_acquire_16_block(ptr, tmp); break;
    case __ATOMIC_RELAXED: __cuda_load_relaxed_16_block(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 2);
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_relaxed_32_block(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.relaxed.cta.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_acquire_32_block(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.acquire.cta.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename _VSTD::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_load_cuda(const volatile type *ptr, type *ret, int memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_acquire_32_block(ptr, tmp); break;
    case __ATOMIC_RELAXED: __cuda_load_relaxed_32_block(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_relaxed_64_block(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.relaxed.cta.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_acquire_64_block(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.acquire.cta.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename _VSTD::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_load_cuda(const volatile type *ptr, type *ret, int memorder, __thread_scope_block_tag) {
    uint64_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_acquire_64_block(ptr, tmp); break;
    case __ATOMIC_RELAXED: __cuda_load_relaxed_64_block(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_relaxed_8_block(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.relaxed.cta.b8 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_release_8_block(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.release.cta.b8 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==1, int>::type = 0>
__device__ void __atomic_store_cuda(volatile type *ptr, type *val, int memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 1);
    switch (memorder) {
    case __ATOMIC_RELEASE: __cuda_store_release_8_block(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_RELAXED: __cuda_store_relaxed_8_block(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_relaxed_16_block(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.relaxed.cta.b16 [%0], %1;" :: "l"(_ptr),"h"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_release_16_block(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.release.cta.b16 [%0], %1;" :: "l"(_ptr),"h"(_src) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==2, int>::type = 0>
__device__ void __atomic_store_cuda(volatile type *ptr, type *val, int memorder, __thread_scope_block_tag) {
    uint16_t tmp = 0;
    memcpy(&tmp, val, 2);
    switch (memorder) {
    case __ATOMIC_RELEASE: __cuda_store_release_16_block(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_RELAXED: __cuda_store_relaxed_16_block(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_relaxed_32_block(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.relaxed.cta.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_release_32_block(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.release.cta.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_store_cuda(volatile type *ptr, type *val, int memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
    case __ATOMIC_RELEASE: __cuda_store_release_32_block(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_RELAXED: __cuda_store_relaxed_32_block(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_relaxed_64_block(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.relaxed.cta.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_release_64_block(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.release.cta.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_store_cuda(volatile type *ptr, type *val, int memorder, __thread_scope_block_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
    case __ATOMIC_RELEASE: __cuda_store_release_64_block(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_RELAXED: __cuda_store_relaxed_64_block(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_relaxed_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.relaxed.cta.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_acquire_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acquire.cta.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_release_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.release.cta.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_acq_rel_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acq_rel.cta.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ bool __atomic_compare_exchange_cuda(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 4);
    memcpy(&old, expected, 4);
    old_tmp = old;
    switch (__stronger_order_cuda(success_memorder, failure_memorder)) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_cas_acquire_32_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_cas_acq_rel_32_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_cas_release_32_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_cas_relaxed_32_block(ptr, old, old_tmp, tmp); break;
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_relaxed_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.relaxed.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_acquire_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acquire.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_release_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.release.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_acq_rel_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acq_rel.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_exchange_cuda(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_exch_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_exch_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_exch_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_exch_relaxed_32_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_relaxed_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.relaxed.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_acquire_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acquire.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_release_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.release.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_acq_rel_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acq_rel.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_add_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_add_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_add_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_add_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_add_relaxed_32_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_relaxed_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.relaxed.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_acquire_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acquire.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_release_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.release.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_acq_rel_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acq_rel.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_and_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_and_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_and_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_and_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_and_relaxed_32_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_relaxed_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.relaxed.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_acquire_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acquire.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_release_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.release.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_acq_rel_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acq_rel.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_or_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_or_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_or_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_or_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_or_relaxed_32_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_sub_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    tmp = -tmp;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_add_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_add_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_add_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_add_relaxed_32_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_relaxed_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.relaxed.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_acquire_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acquire.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_release_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.release.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_acq_rel_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acq_rel.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_xor_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_xor_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_xor_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_xor_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_xor_relaxed_32_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_relaxed_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.relaxed.cta.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_acquire_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acquire.cta.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_release_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.release.cta.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_acq_rel_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acq_rel.cta.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ bool __atomic_compare_exchange_cuda(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_block_tag) {
    uint64_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 8);
    memcpy(&old, expected, 8);
    old_tmp = old;
    switch (__stronger_order_cuda(success_memorder, failure_memorder)) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_cas_acquire_64_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_cas_acq_rel_64_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_cas_release_64_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_cas_relaxed_64_block(ptr, old, old_tmp, tmp); break;
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_relaxed_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.relaxed.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_acquire_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acquire.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_release_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.release.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_acq_rel_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acq_rel.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_exchange_cuda(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_block_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_exch_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_exch_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_exch_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_exch_relaxed_64_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_relaxed_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.relaxed.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_acquire_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acquire.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_release_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.release.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_acq_rel_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acq_rel.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_add_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_add_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_add_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_add_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_add_relaxed_64_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_relaxed_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.relaxed.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_acquire_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acquire.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_release_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.release.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_acq_rel_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acq_rel.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_and_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_and_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_and_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_and_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_and_relaxed_64_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_relaxed_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.relaxed.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_acquire_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acquire.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_release_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.release.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_acq_rel_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acq_rel.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_or_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_or_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_or_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_or_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_or_relaxed_64_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_sub_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp = -tmp;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_add_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_add_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_add_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_add_relaxed_64_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_relaxed_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.relaxed.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_acquire_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acquire.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_release_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.release.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_acq_rel_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acq_rel.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_xor_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_xor_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_xor_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_xor_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_xor_relaxed_64_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_add_cuda(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_block_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp *= sizeof(type);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_add_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_add_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_add_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_add_relaxed_64_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_sub_cuda(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_block_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp = -tmp;
    tmp *= sizeof(type);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_add_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_add_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_add_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_add_relaxed_64_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
static inline __device__ void __cuda_fence_sc_device() { asm volatile("fence.sc.gpu;":::"memory"); }
static inline __device__ void __cuda_fence_acq_rel_device() { asm volatile("fence.acq_rel.gpu;":::"memory"); }
static inline __device__ void __atomic_thread_fence_cuda(int memorder, __thread_scope_device_tag) {
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device(); break;
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE:
    case __ATOMIC_ACQ_REL:
    case __ATOMIC_RELEASE: __cuda_fence_acq_rel_device(); break;
    case __ATOMIC_RELAXED: break;
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_relaxed_8_device(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.relaxed.gpu.b8 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_acquire_8_device(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.acquire.gpu.b8 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename _VSTD::enable_if<sizeof(type)==1, int>::type = 0>
__device__ void __atomic_load_cuda(const volatile type *ptr, type *ret, int memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_acquire_8_device(ptr, tmp); break;
    case __ATOMIC_RELAXED: __cuda_load_relaxed_8_device(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 1);
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_relaxed_16_device(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.relaxed.gpu.b16 %0,[%1];" : "=h"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_acquire_16_device(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.acquire.gpu.b16 %0,[%1];" : "=h"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename _VSTD::enable_if<sizeof(type)==2, int>::type = 0>
__device__ void __atomic_load_cuda(const volatile type *ptr, type *ret, int memorder, __thread_scope_device_tag) {
    uint16_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_acquire_16_device(ptr, tmp); break;
    case __ATOMIC_RELAXED: __cuda_load_relaxed_16_device(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 2);
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_relaxed_32_device(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.relaxed.gpu.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_acquire_32_device(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.acquire.gpu.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename _VSTD::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_load_cuda(const volatile type *ptr, type *ret, int memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_acquire_32_device(ptr, tmp); break;
    case __ATOMIC_RELAXED: __cuda_load_relaxed_32_device(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_relaxed_64_device(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.relaxed.gpu.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_acquire_64_device(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.acquire.gpu.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename _VSTD::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_load_cuda(const volatile type *ptr, type *ret, int memorder, __thread_scope_device_tag) {
    uint64_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_acquire_64_device(ptr, tmp); break;
    case __ATOMIC_RELAXED: __cuda_load_relaxed_64_device(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_relaxed_8_device(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.relaxed.gpu.b8 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_release_8_device(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.release.gpu.b8 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==1, int>::type = 0>
__device__ void __atomic_store_cuda(volatile type *ptr, type *val, int memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 1);
    switch (memorder) {
    case __ATOMIC_RELEASE: __cuda_store_release_8_device(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_RELAXED: __cuda_store_relaxed_8_device(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_relaxed_16_device(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.relaxed.gpu.b16 [%0], %1;" :: "l"(_ptr),"h"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_release_16_device(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.release.gpu.b16 [%0], %1;" :: "l"(_ptr),"h"(_src) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==2, int>::type = 0>
__device__ void __atomic_store_cuda(volatile type *ptr, type *val, int memorder, __thread_scope_device_tag) {
    uint16_t tmp = 0;
    memcpy(&tmp, val, 2);
    switch (memorder) {
    case __ATOMIC_RELEASE: __cuda_store_release_16_device(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_RELAXED: __cuda_store_relaxed_16_device(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_relaxed_32_device(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.relaxed.gpu.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_release_32_device(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.release.gpu.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_store_cuda(volatile type *ptr, type *val, int memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
    case __ATOMIC_RELEASE: __cuda_store_release_32_device(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_RELAXED: __cuda_store_relaxed_32_device(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_relaxed_64_device(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.relaxed.gpu.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_release_64_device(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.release.gpu.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_store_cuda(volatile type *ptr, type *val, int memorder, __thread_scope_device_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
    case __ATOMIC_RELEASE: __cuda_store_release_64_device(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_RELAXED: __cuda_store_relaxed_64_device(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_relaxed_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.relaxed.gpu.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_acquire_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acquire.gpu.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_release_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.release.gpu.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_acq_rel_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acq_rel.gpu.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ bool __atomic_compare_exchange_cuda(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 4);
    memcpy(&old, expected, 4);
    old_tmp = old;
    switch (__stronger_order_cuda(success_memorder, failure_memorder)) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_cas_acquire_32_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_cas_acq_rel_32_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_cas_release_32_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_cas_relaxed_32_device(ptr, old, old_tmp, tmp); break;
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_relaxed_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.relaxed.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_acquire_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acquire.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_release_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.release.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_acq_rel_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acq_rel.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_exchange_cuda(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_exch_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_exch_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_exch_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_exch_relaxed_32_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_relaxed_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.relaxed.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_acquire_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acquire.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_release_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.release.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_acq_rel_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acq_rel.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_add_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_add_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_add_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_add_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_add_relaxed_32_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_relaxed_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.relaxed.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_acquire_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acquire.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_release_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.release.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_acq_rel_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acq_rel.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_and_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_and_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_and_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_and_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_and_relaxed_32_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_relaxed_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.relaxed.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_acquire_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acquire.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_release_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.release.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_acq_rel_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acq_rel.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_or_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_or_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_or_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_or_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_or_relaxed_32_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_sub_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    tmp = -tmp;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_add_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_add_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_add_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_add_relaxed_32_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_relaxed_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.relaxed.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_acquire_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acquire.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_release_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.release.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_acq_rel_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acq_rel.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_xor_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_xor_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_xor_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_xor_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_xor_relaxed_32_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_relaxed_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.relaxed.gpu.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_acquire_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acquire.gpu.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_release_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.release.gpu.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_acq_rel_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acq_rel.gpu.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ bool __atomic_compare_exchange_cuda(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_device_tag) {
    uint64_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 8);
    memcpy(&old, expected, 8);
    old_tmp = old;
    switch (__stronger_order_cuda(success_memorder, failure_memorder)) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_cas_acquire_64_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_cas_acq_rel_64_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_cas_release_64_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_cas_relaxed_64_device(ptr, old, old_tmp, tmp); break;
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_relaxed_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.relaxed.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_acquire_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acquire.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_release_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.release.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_acq_rel_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acq_rel.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_exchange_cuda(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_device_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_exch_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_exch_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_exch_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_exch_relaxed_64_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_relaxed_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.relaxed.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_acquire_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acquire.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_release_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.release.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_acq_rel_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acq_rel.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_add_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_add_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_add_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_add_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_add_relaxed_64_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_relaxed_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.relaxed.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_acquire_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acquire.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_release_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.release.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_acq_rel_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acq_rel.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_and_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_and_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_and_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_and_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_and_relaxed_64_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_relaxed_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.relaxed.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_acquire_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acquire.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_release_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.release.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_acq_rel_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acq_rel.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_or_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_or_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_or_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_or_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_or_relaxed_64_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_sub_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp = -tmp;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_add_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_add_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_add_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_add_relaxed_64_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_relaxed_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.relaxed.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_acquire_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acquire.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_release_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.release.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_acq_rel_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acq_rel.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_xor_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_xor_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_xor_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_xor_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_xor_relaxed_64_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_add_cuda(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_device_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp *= sizeof(type);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_add_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_add_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_add_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_add_relaxed_64_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_sub_cuda(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_device_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp = -tmp;
    tmp *= sizeof(type);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_add_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_add_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_add_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_add_relaxed_64_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
static inline __device__ void __cuda_fence_sc_system() { asm volatile("fence.sc.sys;":::"memory"); }
static inline __device__ void __cuda_fence_acq_rel_system() { asm volatile("fence.acq_rel.sys;":::"memory"); }
static inline __device__ void __atomic_thread_fence_cuda(int memorder, __thread_scope_system_tag) {
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system(); break;
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE:
    case __ATOMIC_ACQ_REL:
    case __ATOMIC_RELEASE: __cuda_fence_acq_rel_system(); break;
    case __ATOMIC_RELAXED: break;
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_relaxed_8_system(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.relaxed.sys.b8 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_acquire_8_system(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.acquire.sys.b8 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename _VSTD::enable_if<sizeof(type)==1, int>::type = 0>
__device__ void __atomic_load_cuda(const volatile type *ptr, type *ret, int memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_acquire_8_system(ptr, tmp); break;
    case __ATOMIC_RELAXED: __cuda_load_relaxed_8_system(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 1);
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_relaxed_16_system(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.relaxed.sys.b16 %0,[%1];" : "=h"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_acquire_16_system(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.acquire.sys.b16 %0,[%1];" : "=h"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename _VSTD::enable_if<sizeof(type)==2, int>::type = 0>
__device__ void __atomic_load_cuda(const volatile type *ptr, type *ret, int memorder, __thread_scope_system_tag) {
    uint16_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_acquire_16_system(ptr, tmp); break;
    case __ATOMIC_RELAXED: __cuda_load_relaxed_16_system(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 2);
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_relaxed_32_system(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.relaxed.sys.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_acquire_32_system(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.acquire.sys.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename _VSTD::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_load_cuda(const volatile type *ptr, type *ret, int memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_acquire_32_system(ptr, tmp); break;
    case __ATOMIC_RELAXED: __cuda_load_relaxed_32_system(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_relaxed_64_system(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.relaxed.sys.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_acquire_64_system(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.acquire.sys.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename _VSTD::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_load_cuda(const volatile type *ptr, type *ret, int memorder, __thread_scope_system_tag) {
    uint64_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_acquire_64_system(ptr, tmp); break;
    case __ATOMIC_RELAXED: __cuda_load_relaxed_64_system(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_relaxed_8_system(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.relaxed.sys.b8 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_release_8_system(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.release.sys.b8 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==1, int>::type = 0>
__device__ void __atomic_store_cuda(volatile type *ptr, type *val, int memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 1);
    switch (memorder) {
    case __ATOMIC_RELEASE: __cuda_store_release_8_system(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_RELAXED: __cuda_store_relaxed_8_system(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_relaxed_16_system(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.relaxed.sys.b16 [%0], %1;" :: "l"(_ptr),"h"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_release_16_system(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.release.sys.b16 [%0], %1;" :: "l"(_ptr),"h"(_src) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==2, int>::type = 0>
__device__ void __atomic_store_cuda(volatile type *ptr, type *val, int memorder, __thread_scope_system_tag) {
    uint16_t tmp = 0;
    memcpy(&tmp, val, 2);
    switch (memorder) {
    case __ATOMIC_RELEASE: __cuda_store_release_16_system(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_RELAXED: __cuda_store_relaxed_16_system(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_relaxed_32_system(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.relaxed.sys.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_release_32_system(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.release.sys.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_store_cuda(volatile type *ptr, type *val, int memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
    case __ATOMIC_RELEASE: __cuda_store_release_32_system(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_RELAXED: __cuda_store_relaxed_32_system(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_relaxed_64_system(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.relaxed.sys.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_release_64_system(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.release.sys.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_store_cuda(volatile type *ptr, type *val, int memorder, __thread_scope_system_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
    case __ATOMIC_RELEASE: __cuda_store_release_64_system(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_RELAXED: __cuda_store_relaxed_64_system(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_relaxed_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.relaxed.sys.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_acquire_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acquire.sys.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_release_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.release.sys.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_acq_rel_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acq_rel.sys.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ bool __atomic_compare_exchange_cuda(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 4);
    memcpy(&old, expected, 4);
    old_tmp = old;
    switch (__stronger_order_cuda(success_memorder, failure_memorder)) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_cas_acquire_32_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_cas_acq_rel_32_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_cas_release_32_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_cas_relaxed_32_system(ptr, old, old_tmp, tmp); break;
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_relaxed_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.relaxed.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_acquire_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acquire.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_release_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.release.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_acq_rel_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acq_rel.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_exchange_cuda(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_exch_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_exch_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_exch_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_exch_relaxed_32_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_relaxed_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.relaxed.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_acquire_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acquire.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_release_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.release.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_acq_rel_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acq_rel.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_add_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_add_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_add_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_add_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_add_relaxed_32_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_relaxed_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.relaxed.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_acquire_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acquire.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_release_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.release.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_acq_rel_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acq_rel.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_and_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_and_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_and_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_and_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_and_relaxed_32_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_relaxed_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.relaxed.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_acquire_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acquire.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_release_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.release.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_acq_rel_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acq_rel.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_or_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_or_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_or_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_or_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_or_relaxed_32_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_sub_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    tmp = -tmp;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_add_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_add_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_add_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_add_relaxed_32_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_relaxed_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.relaxed.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_acquire_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acquire.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_release_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.release.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_acq_rel_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acq_rel.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_xor_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_xor_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_xor_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_xor_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_xor_relaxed_32_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_relaxed_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.relaxed.sys.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_acquire_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acquire.sys.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_release_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.release.sys.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_cas_acq_rel_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acq_rel.sys.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ bool __atomic_compare_exchange_cuda(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_system_tag) {
    uint64_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 8);
    memcpy(&old, expected, 8);
    old_tmp = old;
    switch (__stronger_order_cuda(success_memorder, failure_memorder)) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_cas_acquire_64_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_cas_acq_rel_64_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_cas_release_64_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_cas_relaxed_64_system(ptr, old, old_tmp, tmp); break;
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_relaxed_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.relaxed.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_acquire_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acquire.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_release_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.release.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exch_acq_rel_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acq_rel.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_exchange_cuda(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_system_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_exch_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_exch_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_exch_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_exch_relaxed_64_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_relaxed_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.relaxed.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_acquire_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acquire.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_release_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.release.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_add_acq_rel_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acq_rel.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_add_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_add_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_add_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_add_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_add_relaxed_64_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_relaxed_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.relaxed.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_acquire_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acquire.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_release_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.release.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_and_acq_rel_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acq_rel.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_and_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_and_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_and_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_and_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_and_relaxed_64_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_relaxed_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.relaxed.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_acquire_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acquire.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_release_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.release.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_or_acq_rel_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acq_rel.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_or_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_or_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_or_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_or_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_or_relaxed_64_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_sub_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp = -tmp;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_add_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_add_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_add_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_add_relaxed_64_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_relaxed_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.relaxed.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_acquire_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acquire.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_release_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.release.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_xor_acq_rel_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acq_rel.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_xor_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_xor_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_xor_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_xor_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_xor_relaxed_64_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_add_cuda(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_system_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp *= sizeof(type);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_add_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_add_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_add_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_add_relaxed_64_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_sub_cuda(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_system_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp = -tmp;
    tmp *= sizeof(type);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_add_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_add_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_add_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_add_relaxed_64_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}

}
_LIBCPP_END_NAMESPACE_CUDA
